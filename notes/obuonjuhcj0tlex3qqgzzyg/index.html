<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-main/favicon.ico"/><title>Intro to LLMs</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Knowledge Space"/><meta property="og:title" content="Intro to LLMs"/><meta property="og:description" content="Personal Knowledge Space"/><meta property="og:url" content="https://sid1081.github.io/dendron-main/notes/obuonjuhcj0tlex3qqgzzyg/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2/13/2025"/><meta property="article:modified_time" content="2/19/2025"/><link rel="canonical" href="https://sid1081.github.io/dendron-main/notes/obuonjuhcj0tlex3qqgzzyg/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-main/_next/static/css/c3b5c8984096a078.css" as="style"/><link rel="stylesheet" href="/dendron-main/_next/static/css/c3b5c8984096a078.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-main/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-main/_next/static/chunks/webpack-0caed3b86e25c9d3.js" defer=""></script><script src="/dendron-main/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-main/_next/static/chunks/main-af01d34aaf1cbbc0.js" defer=""></script><script src="/dendron-main/_next/static/chunks/pages/_app-f3ffa24acc1f5021.js" defer=""></script><script src="/dendron-main/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-main/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-main/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-main/_next/static/j1tAvk9N20cOpobDNsQ-j/_buildManifest.js" defer=""></script><script src="/dendron-main/_next/static/j1tAvk9N20cOpobDNsQ-j/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="intro-to-llms"><a aria-hidden="true" class="anchor-heading icon-link" href="#intro-to-llms"></a>Intro to LLMs</h1>
<h3 id="intro-to-llms-1"><a aria-hidden="true" class="anchor-heading icon-link" href="#intro-to-llms-1"></a>Intro to LLMS</h3>
<ul>
<li>
<p>A LLM is just two files - a parameter file(140GB) and an executable(500 lines of code), eg: Llama-2-70B is the Lllama 2.0 model released by Meta which is trained on 70 B paramters. </p>
</li>
<li>
<p>The parameteres are essentially the weights or the paramaters of this neural network that is a LLM.</p>
</li>
<li>
<p>Llama is an open source; the weights, the architecture and the executable were released by Meta.</p>
</li>
<li>
<p>Chat GPT never released the model architecture so you don't have access to that model.</p>
</li>
<li>
<p>Model inference is when you just run the model on your macbook.</p>
</li>
<li>
<p>Model training is a lot more involved.</p>
</li>
<li>
<p>This LLAMA model is essentially just a next word prediction model. This might seem trivial, but being able to predic the next word(s) can be very powerful since if the objective is to be a next word predicion model, eg to be able to predict the next word about Ruth Handler, you need to parse a lot of information and you end up /learning/ a lot in the process. <em>All of this knowledge is being compressed into the weights of the neural network ??</em>. - The neural network isn't actually memorizing the data but instead storing complicated patterns and learning deeper patterns and each weight contributes to understanding multiple concepts.</p>
</li>
<li>
<p>First, about "inference" - this just means using the trained model to generate text, as opposed to training it. When you use ChatGPT or when Claude responds to you, that's inference.</p>
</li>
<li>
<p>The text generation process works like this:</p>
<ul>
<li>
<p>The model looks at the input text (like your question)</p>
</li>
<li>
<p>It predicts what word is most likely to come next</p>
</li>
<li>
<p>It adds that word to the sequence</p>
</li>
<li>
<p>Then it uses the updated sequence to predict the next word</p>
</li>
<li>
<p>This process repeats until it completes the response</p>
</li>
<li>
<p>The phrase "dreams internet documents" is a colorful way of saying that the model recreates text patterns similar to its training data (which includes many internet documents). It's not literally dreaming or remembering specific documents - instead, it has learned patterns from millions of texts and can generate new text following similar patterns.</p>
</li>
</ul>
</li>
</ul>
<h3 id="there-are-three-main-parts-of-building-a-model---training-inference-and-fine-tuning"><a aria-hidden="true" class="anchor-heading icon-link" href="#there-are-three-main-parts-of-building-a-model---training-inference-and-fine-tuning"></a>There are three main parts of building a model - <strong>training</strong>, <strong>inference</strong> and <strong>fine-tuning</strong>.</h3>
<pre><code>- Training involves firstly scraping the internet for loads of text, of the magnitude of 10TB. After scraping all of this data, the model is trained on this data and all of this data is then _compressed_ into the paramters and weights of the neural network. The LLAMA 2.0 model for instance has 70B parameters which is around 140GB of data. So you can think that the compression that takes place is some sort of lossy compression where all of the data is sort of compressed into these parameters. Training is the most expensive part of the process where a huge GPU cluster is leveraged for over 12 days(for LLAMA) and costs 2M dollars.

- Inference is the most straightforward part. When interacting with a model, the model performs inference and responds to your queries.

- Fine-Tuning is the step after inference where the model turns from being a next word prediction model to some sort of assistant.
</code></pre>
<h3 id="how-does-it-work"><a aria-hidden="true" class="anchor-heading icon-link" href="#how-does-it-work"></a>How does it work?</h3>
<pre><code>- We don't know really. We know in full detail what the architecture of the neural network looks like and what math is performed at each step, but we still don't know _how_ these parameters/nodes which are dispersed through the neural network interact with each other.

- We know that they build some sort of knowledge base but it's a bit strange. We know that it works _emperically_, and we know how to fine-tune models and make them better based on evaluation tests.
</code></pre>
<h3 id="fine-tuning"><a aria-hidden="true" class="anchor-heading icon-link" href="#fine-tuning"></a>Fine-Tuning</h3>
<pre><code>- We first come up with the labeling instructions for the fine tuning job, and then hire people or use scale.ai to perform these labeling tasks where they maybe compile 100,000 questions and answer type forms.

- These forms are very high quality.

- The model is then trained on these forms(training takes approx 1 day) and it then subscribes to the structure of these forms and turns in to an assistant.

- You can then run a lot of evalutaions on this assistant and go back to the first step again to imporove the model based on your evaluations.
</code></pre>
<h3 id="scaling-laws"><a aria-hidden="true" class="anchor-heading icon-link" href="#scaling-laws"></a>Scaling Laws</h3>
<pre><code>- The performance of the model is governed by N - **number of parameters in the network** and `D` - **amount of text** it's trained on.

- So far, there is not upper bound on the performance so in theory we could just keep scaling.

- Algorithmic improvements would be a plus but in theory we can just keep getting better with more **time** and more **compute**.
</code></pre>
<h3 id="tool-use"><a aria-hidden="true" class="anchor-heading icon-link" href="#tool-use"></a>Tool use</h3>
<pre><code>- Tool use is another reason why these LLMs are becoming so capable. Being able to use tools like browsers, calculators, graphing libraries, image generation libraries etc empowers the LLMS to perform complex tasks like humans would.
</code></pre>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/dendron-main/notes/kndcs9cayrizdzefp1veykh">AK</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#intro-to-llms" title="Intro to LLMS">Intro to LLMS</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#there-are-three-main-parts-of-building-a-model---training-inference-and-fine-tuning" title="There are three main parts of building a model - training, inference and fine-tuning.">There are three main parts of building a model - training, inference and fine-tuning.</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#how-does-it-work" title="How does it work?">How does it work?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#fine-tuning" title="Fine-Tuning">Fine-Tuning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#scaling-laws" title="Scaling Laws">Scaling Laws</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tool-use" title="Tool use">Tool use</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"obuonjuhcj0tlex3qqgzzyg","title":"Intro to LLMs","desc":"","updated":1740001542785,"created":1739407566715,"url":"https://www.youtube.com/watch?v=zjkBMFhNj_g\u0026ab_channel=AndrejKarpathy","custom":{"url":"https://www.youtube.com/watch?v=zjkBMFhNj_g\u0026ab_channel=AndrejKarpathy"},"fname":"ML.AK.Intro to LLMs","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"37ab0a87b633c1c6b234c87f7d647983","links":[{"from":{"fname":"ML.AK","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":3,"column":42,"offset":115},"end":{"line":3,"column":65,"offset":138},"indent":[]},"value":"ML.AK.Intro to LLMs","alias":"ML.AK.Intro to LLMs"}],"anchors":{"intro-to-llms":{"type":"header","text":"Intro to LLMS","value":"intro-to-llms","line":9,"column":0,"depth":3},"there-are-three-main-parts-of-building-a-model---training-inference-and-fine-tuning":{"type":"header","text":"There are three main parts of building a model - training, inference and fine-tuning.","value":"there-are-three-main-parts-of-building-a-model---training-inference-and-fine-tuning","line":41,"column":0,"depth":3},"how-does-it-work":{"type":"header","text":"How does it work?","value":"how-does-it-work","line":49,"column":0,"depth":3},"fine-tuning":{"type":"header","text":"Fine-Tuning","value":"fine-tuning","line":55,"column":0,"depth":3},"scaling-laws":{"type":"header","text":"Scaling Laws","value":"scaling-laws","line":65,"column":0,"depth":3},"tool-use":{"type":"header","text":"Tool use","value":"tool-use","line":73,"column":0,"depth":3}},"children":[],"parent":"kndcs9cayrizdzefp1veykh","data":{}},"body":"\u003ch1 id=\"intro-to-llms\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#intro-to-llms\"\u003e\u003c/a\u003eIntro to LLMs\u003c/h1\u003e\n\u003ch3 id=\"intro-to-llms-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#intro-to-llms-1\"\u003e\u003c/a\u003eIntro to LLMS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eA LLM is just two files - a parameter file(140GB) and an executable(500 lines of code), eg: Llama-2-70B is the Lllama 2.0 model released by Meta which is trained on 70 B paramters. \u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe parameteres are essentially the weights or the paramaters of this neural network that is a LLM.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLlama is an open source; the weights, the architecture and the executable were released by Meta.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eChat GPT never released the model architecture so you don't have access to that model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eModel inference is when you just run the model on your macbook.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eModel training is a lot more involved.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis LLAMA model is essentially just a next word prediction model. This might seem trivial, but being able to predic the next word(s) can be very powerful since if the objective is to be a next word predicion model, eg to be able to predict the next word about Ruth Handler, you need to parse a lot of information and you end up /learning/ a lot in the process. \u003cem\u003eAll of this knowledge is being compressed into the weights of the neural network ??\u003c/em\u003e. - The neural network isn't actually memorizing the data but instead storing complicated patterns and learning deeper patterns and each weight contributes to understanding multiple concepts.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFirst, about \"inference\" - this just means using the trained model to generate text, as opposed to training it. When you use ChatGPT or when Claude responds to you, that's inference.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe text generation process works like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe model looks at the input text (like your question)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIt predicts what word is most likely to come next\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIt adds that word to the sequence\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThen it uses the updated sequence to predict the next word\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis process repeats until it completes the response\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe phrase \"dreams internet documents\" is a colorful way of saying that the model recreates text patterns similar to its training data (which includes many internet documents). It's not literally dreaming or remembering specific documents - instead, it has learned patterns from millions of texts and can generate new text following similar patterns.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"there-are-three-main-parts-of-building-a-model---training-inference-and-fine-tuning\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#there-are-three-main-parts-of-building-a-model---training-inference-and-fine-tuning\"\u003e\u003c/a\u003eThere are three main parts of building a model - \u003cstrong\u003etraining\u003c/strong\u003e, \u003cstrong\u003einference\u003c/strong\u003e and \u003cstrong\u003efine-tuning\u003c/strong\u003e.\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e- Training involves firstly scraping the internet for loads of text, of the magnitude of 10TB. After scraping all of this data, the model is trained on this data and all of this data is then _compressed_ into the paramters and weights of the neural network. The LLAMA 2.0 model for instance has 70B parameters which is around 140GB of data. So you can think that the compression that takes place is some sort of lossy compression where all of the data is sort of compressed into these parameters. Training is the most expensive part of the process where a huge GPU cluster is leveraged for over 12 days(for LLAMA) and costs 2M dollars.\n\n- Inference is the most straightforward part. When interacting with a model, the model performs inference and responds to your queries.\n\n- Fine-Tuning is the step after inference where the model turns from being a next word prediction model to some sort of assistant.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"how-does-it-work\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-does-it-work\"\u003e\u003c/a\u003eHow does it work?\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e- We don't know really. We know in full detail what the architecture of the neural network looks like and what math is performed at each step, but we still don't know _how_ these parameters/nodes which are dispersed through the neural network interact with each other.\n\n- We know that they build some sort of knowledge base but it's a bit strange. We know that it works _emperically_, and we know how to fine-tune models and make them better based on evaluation tests.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"fine-tuning\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#fine-tuning\"\u003e\u003c/a\u003eFine-Tuning\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e- We first come up with the labeling instructions for the fine tuning job, and then hire people or use scale.ai to perform these labeling tasks where they maybe compile 100,000 questions and answer type forms.\n\n- These forms are very high quality.\n\n- The model is then trained on these forms(training takes approx 1 day) and it then subscribes to the structure of these forms and turns in to an assistant.\n\n- You can then run a lot of evalutaions on this assistant and go back to the first step again to imporove the model based on your evaluations.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"scaling-laws\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#scaling-laws\"\u003e\u003c/a\u003eScaling Laws\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e- The performance of the model is governed by N - **number of parameters in the network** and `D` - **amount of text** it's trained on.\n\n- So far, there is not upper bound on the performance so in theory we could just keep scaling.\n\n- Algorithmic improvements would be a plus but in theory we can just keep getting better with more **time** and more **compute**.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"tool-use\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tool-use\"\u003e\u003c/a\u003eTool use\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e- Tool use is another reason why these LLMs are becoming so capable. Being able to use tools like browsers, calculators, graphing libraries, image generation libraries etc empowers the LLMS to perform complex tasks like humans would.\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/dendron-main/notes/kndcs9cayrizdzefp1veykh\"\u003eAK\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"root","title":"root","desc":"","updated":1746040151076,"created":1595961348801,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"3338f3f44ea53fff248fd70b6ad85170","links":[],"anchors":{},"children":["oz8e6a05bjgd3cbtru3hqe2","c1bs7wsjfbhb0zipaywqv1","3x4fmd16chquma6kovvfatj","gdjyzsq56qgyj5t5uktrsgu","69h59jt3vyfdg55g7gqc95d","wz9z8az9m9aq5132p7d10co","l1lcmphk4ubvu1aiqhcs7ib","bd3mupfj68nrgnjejss1qqh","1tlnuf6o2f3qawzg799ilzk","z5mfdumpr0cv3ceh60flq8n","filwymswyiwdojrwinvy5xd"],"parent":null,"data":{},"body":"\nSiddharth's root\n\nThis vault contains all the verticals to climb."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false,"enableHandlebarTemplates":true,"enableSmartRefs":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableMermaid":true},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal Knowledge Space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"search","siteUrl":"https://sid1081.github.io","assetsPrefix":"/dendron-main","enableMermaid":true,"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"obuonjuhcj0tlex3qqgzzyg"},"buildId":"j1tAvk9N20cOpobDNsQ-j","assetPrefix":"/dendron-main","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>