{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"root","n":1},"1":{"v":"\nSiddharth's root\n\nThis vault contains all the verticals to climb.","n":0.333}}},{"i":2,"$":{"0":{"v":"Templates","n":1}}},{"i":3,"$":{"0":{"v":"Daily Journal Template","n":0.577},"1":{"v":"This template was applied using the daily journal schema. Edit the [[templates.daily]] note to change this template.\nTo create your own schemas to auto-apply templates when they match a hierarchy, follow the [schema tutorial](https://blog.dendron.so/notes/P1DL2uXHpKUCa7hLiFbFA/) to get started.\n\n<!--\nBased on the journaling method created by Intelligent Change:\n- [Intelligent Change: Our Story](https://www.intelligentchange.com/pages/our-story)\n- [The Five Minute Journal](https://www.intelligentchange.com/products/the-five-minute-journal)\n-->\n\n## Morning\n\n<!-- Fill out this section after waking up -->\n\n### Gratitude\n\nI am grateful for:\n\n1.\n2.\n3.\n\n### What would make today great?\n\n1.\n2.\n3.\n\n### Daily affirmations\n\nI am...\n\n## Evening\n\n<!-- Fill out this section before going to sleep, reflecting on your day -->\n\n### Amazing things that happened today\n\n1.\n2.\n3.\n\n### How could I have made today even better?\n\nI could have made today better by\n","n":0.097}}},{"i":4,"$":{"0":{"v":"Task","n":1}}},{"i":5,"$":{"0":{"v":"2025","n":1}}},{"i":6,"$":{"0":{"v":"05","n":1}}},{"i":7,"$":{"0":{"v":"07","n":1}}},{"i":8,"$":{"0":{"v":"SP","n":1}}},{"i":9,"$":{"0":{"v":"Messager Rendere","n":0.707},"1":{"v":"\n- [x] Test which assistant is generating the right msgJSON response.\n- [x] Test Alici's method for storing the chunks.\n- [x] Test Claude's capabilities for changing the msgJSON.\n- [x] Test changes in the msgJSON like changing a button etc.\n- [X] Render the message in the rendering app to see if it works.","n":0.14}}},{"i":10,"$":{"0":{"v":"Journal","n":1}}},{"i":11,"$":{"0":{"v":"2025","n":1}}},{"i":12,"$":{"0":{"v":"05","n":1}}},{"i":13,"$":{"0":{"v":"2025-05-07","n":1},"1":{"v":" \n- [>] ! Dismantle the couch\n- [>] ! Record notes for 3Blue1Brown\n- [x] ! Arrange 4 mins of your track\n- [x] ! Make progressss on the AWS Project\n- [x] ! Workout\n- [x] Take a walk and buy:\n  - [x] mint\n  - [x] milk\n  - [x] sweet potatoes\n\n|     Tracker | Task                                     | Backlog                        |\n| ----------: | :--------------------------------------- | :----------------------------- |\n| 13:30-15:30 | [[task.2025.05.07.SP.Messager Renderer]] | [[SP.Message Renderer.Step 1]] |\n|             |                                          |                                |\n\n|        Time | Block                                 |\n| ----------: | :------------------------------------ |\n|     (11:30) | (Revision #1)                         |\n|             |                                       |\n| 11:30-12:00 | AWS Meeting                           |\n|             |                                       |\n| 13:30-15:30 | Deep Work (#1)                        |\n|             | - Focus on finishing up the AWS setup |\n| 16:30-18:30 | Deep Work (#2)                        |\n|             | - Focus on finishing the track        |","n":0.09}}},{"i":14,"$":{"0":{"v":"MSAI UPenn Application","n":0.577},"1":{"v":"\n[X] Work on [[Siddharth.UPenn.SOP]]\n[X] Finish up Advanced Machine learning course\n[X] Update Resume with AI project and Coursera certificate\n[X] File the MSAI Application","n":0.213}}},{"i":15,"$":{"0":{"v":"TS","n":1}}},{"i":16,"$":{"0":{"v":"Setup","n":1},"1":{"v":"\n# Setup for Node for Typescript\n\n```\nmkdir my-node-app\ncd my-node-app/\ngit init\nnpm init -y\nnpm i -D typescript ts-node @types/node\nnpx tsc --init\n```\n\n# Package.json setup\n\n```\n\"build\": \"tsc\",\n\"dev\": \"node --env-file=.env --watch -r ts-node/register src/index.ts\",\n```\n\n","n":0.192}}},{"i":17,"$":{"0":{"v":"Siddharth","n":1}}},{"i":18,"$":{"0":{"v":"UPenn","n":1}}},{"i":19,"$":{"0":{"v":"SOP","n":1},"1":{"v":"# Statement of Purpose - MS in Artificial Intelligence\n## University of Pennsylvania - Engineering\n\nThe evolution of my career from software engineering to AI has been fueled by an unwavering commitment to creating transformative user experiences. As a Principal Engineer at Sourcepoint Technologies, a privacy-tech company in NYC, I've spent years developing high-performing distributed systems leveraged by global enterprises like CNN, IBM, Byte-Dance, A&E Networks, BBC, WSJ, and Sky Group. I lead the core-services team at Sourcepoint, and we are responsible for developing and maintaining our microservices that receive up to 300,000 requests per second. Our services are leveraged by some of the biggest enterprises and publishers worldwide to monetize their online experience through ad revenue. This experience has given me valuable insights into software architecture, system design, distributed systems, distributed logging, distributed metrics, high-scale monitoring, and engineering leadership—skills I now seek to complement with formal education in machine learning and artificial intelligence through UPenn's MS-AI program.\n\n### Professional Background and Transition to AI\n\nMy interest in machine learning began organically through my work. As technology evolved, I noticed a shift in how we could approach software problems. In early 2023, I began systematically studying machine learning, starting with Andrew Ng's courses on Coursera to establish a foundation in linear and logistic regression, neural networks, decision trees, and ensemble methods. I supplemented this with statistical knowledge from OpenStax Statistics (Rice University) and deepened my understanding by studying seminal papers like \"Attention is All You Need,\" learning about transformer architectures, attention mechanisms, and backpropagation through YouTube videos and free courses.\n\nThis self-directed education enabled me to conceptualize and lead Sourcepoint's first AI/ML project—leveraging foundation models to generate consent messages (our proprietary format for representing HTML, CSS, and JS). The project aims to dramatically reduce setup time for our clients from days to minutes. Working on this initiative required me to develop skills in prompt engineering and Retrieval-Augmented Generation (RAG) to inject relevant context into foundation models.\n\n### Why AI Now?\n\nWhile these advancements in AI have enabled software engineers like me to build intelligent applications without developing the models ourselves, this very democratization has revealed limitations in my knowledge. The more I work with these models, the more I recognize the value of understanding their fundamental architecture and training methodologies rather than treating them as managed black boxes through the OpenAI API or Amazon Bedrock.\n\nI find myself increasingly constrained by not having the expertise to build or fine-tune models that could better serve specific use cases. There's a substantial difference between being an AI engineer who implements existing models versus being a machine learning engineer who understands the mathematical foundations and can create tailored solutions. This realization is the primary motivation for my pursuit of further education.\n\n### Why UPenn's MS-AI Program?\n\nAfter researching programs at Georgia Tech, UT Austin, and UIUC, UPenn's curriculum stands out for its strong foundation in core ML/AI concepts. Courses like Principles of Deep Learning, Natural Language Processing, Artificial Intelligence, and GPU Computing for Machine Learning Systems align perfectly with my goal of building a comprehensive theoretical foundation that I can immediately apply in industry to build modern distributed ML systems.\n\nGiven my full-time leadership responsibilities at Sourcepoint, the part-time online format is ideal. I intend to leverage this degree to facilitate a seamless transition from infrastructure engineering to MLOps, combining my production engineering experience with cutting-edge AI knowledge to build more sophisticated, efficient systems.\n\n### Future Vision\n\nWith the MS-AI degree from UPenn, I aim to bridge the gap between traditional software engineering and modern AI systems at Sourcepoint. My immediate goal is to enhance our current AI initiatives by implementing more sophisticated, custom-trained models rather than relying solely on third-party APIs. Long-term, I envision building AI systems that can autonomously generate and optimize consent experiences based on user behavior data and regulatory requirements, potentially revolutionizing how privacy technology functions in the digital ecosystem.\n\nAt Sourcepoint, we are also looking to build models to detect privacy compliance issues for our clients across their various applications. These models would analyze user interfaces, data flows, and backend systems to identify potential violations of regulations like GDPR, CCPA, and emerging privacy laws. By leveraging computer vision and NLP techniques, we could automatically scan websites and applications to detect missing consent notices, improper data collection practices, or unauthorized data sharing. This initiative would transform how companies approach privacy compliance, shifting from reactive manual audits to proactive, continuous monitoring. With the knowledge gained from UPenn's MS-AI program, particularly courses on computer vision and NLP, I would be equipped to lead the development of these specialized models that understand the nuanced requirements of global privacy regulations. This project represents a significant opportunity to apply AI for regulatory technology (\"RegTech\") solutions that could become an industry standard for privacy compliance verification.\n\nMy background in software engineering provides me with the operational knowledge to deploy AI systems at scale, while the MS-AI program will give me the theoretical foundation to build these systems from first principles. This combination will position me to lead the next generation of AI-powered privacy technology solutions.\n\n### Why I Will Succeed in the Program\n\nMy proven ability to create and adhere to rigorous learning schedules will ensure my success in UPenn's MS-AI program. For years, I have maintained a disciplined routine of studying from 6-8 am daily, cycling through topics like statistics, probability, music production, system design, and database engineering quarterly. This self-directed learning approach, combined with my professional experience, enabled me to advance to Principal Engineer at 32 despite a delayed career start.\n\nMy professional background also positions me for success. As someone who leads production systems used by major enterprises, I understand the importance of delivering high-quality solutions within deadlines. This project management experience will be invaluable for balancing coursework with professional responsibilities. Additionally, my experience implementing AI solutions at Sourcepoint has provided practical context that will help me connect theoretical concepts to real-world applications more effectively.\n\n### How I Will Contribute to the MSE-AI Online Community\n\nI plan to contribute to the MSE-AI community in several ways:\n\nFirst, I bring industry perspective from privacy technology, an area where AI applications are increasingly important but must navigate complex regulatory environments. I can share insights on the practical challenges of implementing AI solutions within highly regulated contexts, creating valuable discussion points for peers interested in ethical AI deployment.\n\nSecond, my experience leading engineering teams positions me to contribute meaningfully to collaborative projects. I plan to actively participate in study groups, offering both technical expertise and project management skills to help teams deliver high-quality work efficiently.\n\nThird, I intend to document and share my journey applying program concepts to real-world problems at Sourcepoint. By creating case studies of how I implement course learnings in industry, I can help bridge the gap between academic knowledge and practical application for fellow students.\n\n### Managing the Time Commitment\n\nI am fully prepared to dedicate the required 15+ hours per week per course to this program. My existing schedule already incorporates structured study time that I can expand and adapt. Specifically:\n\nMy current daily routine includes 2 hours of professional development (6-8 am), which I will dedicate entirely to UPenn coursework. I also currently allocate an hour in the evening for additional studying, which I will extend to 2 hours. This provides 21 hours weekly dedicated to coursework.\n\nAt Sourcepoint, I've already discussed this educational pursuit with leadership and secured their support, including flexibility for occasional daytime lectures or team activities that may overlap with work hours. I've also trained team members to handle routine responsibilities, creating additional bandwidth for my studies.\n\nI plan to optimize efficiency by aligning course projects with my professional work wherever possible, applying what I learn directly to Sourcepoint's AI initiatives. This creates a synergistic relationship between my studies and work rather than treating them as competing priorities.\n\nI feel it's important to address the early academic setbacks in my journey. Until 12th grade, I consistently topped every class I was in and achieved a 2350 on my SATs after just two weeks of studying. Growing up in an abusive family environment that severely restricted my freedom, the sudden independence I experienced in college led to poor coping mechanisms. I developed a severe marijuana addiction and eating disorder, which resulted in intestinal ulcers, anorexia, and poor academic performance. This academic failure was particularly difficult to accept given my previous record of excellence. It took me two years of dedicated effort to overcome the addiction and an additional year to heal my body—all while in a foreign country without any family support, which I consider a significant personal achievement. These challenges taught me invaluable lessons about perseverance and self-discipline, and failing early in life was a blessing in disguise.\n\nI believe my determination and discipline, evidenced by my rigorous self-improvement regimen and professional advancement despite these early academic setbacks, will serve me well in UPenn's program. My journey from struggling student to Principal Engineer demonstrates not only resilience but an ability to systematically acquire and apply new knowledge—qualities that will ensure my success in the MS-AI program and enable me to contribute meaningfully to the UPenn community.\n\nI look forward to the opportunity to grow as part of the UPenn community and contribute to advancing the intersection of privacy technology and artificial intelligence.\n","n":0.026}}},{"i":20,"$":{"0":{"v":"Content","n":1}}},{"i":21,"$":{"0":{"v":"Video 3","n":0.707},"1":{"v":"\n- Panner dish\n- Yogurt dish\n- Macro breakdown\n- Ingredients list with places to buy from\n- Total Cost","n":0.25}}},{"i":22,"$":{"0":{"v":"Video 2","n":0.707},"1":{"v":"\n1. Ollama DeepSeek setup.\n    - Ollama setup: https://www.youtube.com/watch?v=oeBDn6vclz0&ab_channel=FahdMirza\n2. MPC server setup for Cline - Whatsapp.\n3. MPC server setup for Cline - Apple Native tools.\n","n":0.204}}},{"i":23,"$":{"0":{"v":"Video 1","n":0.707},"1":{"v":"\n1. Show a list of bullet items.\n2. Group all items by category like schedule and order.\n3. Convert it to to-dos.\n4. Add icons using cline.\n5. Check all grocery items using cline.\n6. Use cline to send an email through apple native tools.\n7. Use cline to send whatsapp message to Roshani.\n8. Use cline to create a calendar event through apple native tools.\n9. Archive to-dos using cline.","n":0.126}}},{"i":24,"$":{"0":{"v":"Architecture","n":1}}},{"i":25,"$":{"0":{"v":"SOP","n":1},"1":{"v":"\nMy name is Siddharth Sharma and I am 32 years old. I have a Bachelors in Computer Science and I am a Principal Engineer at a privacy-tech company in NYC - Sourcepoint Techologies Inc. I live in the hip neighborhood of Willamsburg Brooklyn, NY in a penthouse apartment that I bought in 2023. I have deep expertise in software engineering that I have gained over the years of building production applications from scratch that some of the biggest enterprises and publishers across the world leverage every second to make money off ads on the internet - BBC, WSJ, IBM, SKY, BILD to name a few. Before my time at Sourcepoint I built ETR's research tool from scratch that was leveraged by some of the biggest names on wall st to perform market research and sentiment analysis. This is how my parents or my wife's parents introduce me to new people. This is how society gauges your achievements and deems you worthy of their attention and company.\n\nI'd be lying if I said I wasn't proud of some of these achievements, especially considering my horeendous time at college. Growing up in India, I was a grade A student, always in the 95th percentile, either topping my class or in the top 3. Like most Indian kids at that time, I wasn't really given an option to choose what I wanted to study or to pick my passion. I had a deep passion for learning in general but besides that, I was very fascinated by music(I am eastern clasically trained) and art. I left home at the the age of 17 to start my Bachelors degree in Computer Science at University of Illinois Urbana Champaign. This was a very last min decision fueled from having an abusive grandmother and my parents decided that it would be best for me to leave the house. I studied for the SATs for about 2 weeks and got ~2350. I wasn't even aware of colleges like Stanford and MIT back then and only applied to UIUC because a friend of mine had just been admitted into their aerospace engineering program. I wasn't allowed to hang out with friends affter 8pm and I hadn't ever been to a sleep over in my life since my grandmother did not approve of that and cut to flying from Mumbai to Chicago and thats how it all began. My first semester at UIUC, I experienced freedom like never before and having been curtailed all my life, I abused that freedom wholeheartedly. The next 2 years I battled with severe marijuana addiction to the point that I didn't show up for lectures nor did I submit any assignments, I still scraped through the first few semesters still riding on my prowess from the Indian schooling that I had but that only took me so far. I was chucked out of college in 2013 and I transferred to Florida Institute of Technology. It took me one more year of failing and growing up to finally kick my marijuana addiction in the but. Starting early 2015 I got back to my older self and graduated with a computer science degree in Decemeber 2016 with a CGPA of 3.1 having taken 20 credits in the last semester. Since then, I have only looked to move onwards and forward. Failing early in life was a blessing in disguise which I thankfully came out stronger on the other side and it has definitely contributed to my determination, rigor and mindset in life. \n\nStarting 2016 until 2022, I focused on being the best version of myself, whether it be working out, or taking courses, reading books, parpers or working on projects so I can be the best at whatever I do whether it be my fitness or software engineering. My daily routine looked something like this - Study from 6am to 8am, topics change every quarter, ranging from Statistics, Probability, Physics, Math, Machine Learning, Web Development, Typescript, Rust, System Design, Backend Engineering, Databases etc. I'd then go to the gym from 8AM to 9AM followed but two very intense 4 hour work sessions and then another hour long study session at night. This regime is definitely what got me through the ranks at ETR and Sourcepoint. As a principal engineer at the age of 32 despite have a delayed start to my career is a fairly impressive feat.\n\nEarly this year, I started to delve into Machine Learning beyong the typical developer experience of using Cline or Cursor to write unit and regression tests or performing code refactoring. I first took Andrew Ng's Machine Learning Specialization on Coursera and brushed up on topics like Linear and Logistic Regressions followed by Neural Networks. In the mean time, I also brushed up on my College Statistics by reading through OpenStax Statistics by Rice University. This course was my first real intro to Machine Learning and I was hooked. Upon completing this course I started to read more papers(Attention is all you need being one of the few) and learned about the newer machine learning architectures like Transformers, Attention blocks, Backpropagation and such while also keeping up with this fast paced industry with MCP servers, Agentic workflows...add some more stuff here... This deep dive helped me ideate the first AI/ML project at Sourcepoint(which I am currently leading)- leverage foundation models to generate consent_messages(just a proprietary format for representign HTML, CSS and JS). The main aim of this project is to significantly reduce the time and effort it takes our clients to setup consent messages by leveraging foundation models to take them from 0-80 in a matter of a few minutes. In order to work on this project, I had to learn a mix of prompt engineering along RAG methodologies to inject the relevant context into the Foundation models. The boom in AI has allowed sfotware engineers like me with limited ML knowledge to be able to build AI applications without actully developing/training/fine-tuning/evaluating the models myself. Its redcued the barrier for entry to the ML world by democratizing these models. While working with these models has been extremely beneficial us as a company, the more I work with these models, the more I wish that I had the foundation ML skills to build these models myself. While I am not particularly bullish about AI changing the world over night; I do believe that over time(literally since there is no upper limit on AI performance) it's going to be essential to know how to work with these models and this is one of the main reasons I have decided to pursue a Masters In ML/AI. I am tired of using managed blackboxes(Open AI API, Amazon Bedrock) to build applications that would have otherwise been seemless/easier to build using much smaller and simpler models that I could build myself if I had the chops. Write more about how ML Engineering is superlative to AI engineering here...\n\nConsidering that I will be working full time leading the engineering efforts at Sourcepoint, a part-time online degree is my only option. But I am fortunate that we live in a world where Ivy League institutions like UPenn offer such degrees. I have checked out the course catalog of a few universities like Georgia Tech, UT Austin and UIUC's but UPenn's program seems to have a lot of core courses like Principles of Deep Learning, Natural Language Processing, Artifical Intelligence, Computer Vision and GPU Computing for Machine Learning Systems will offer the perfect foundational knowledge that I will be able to leverage in the real world. ....they said the degree was geared towards a ML Ops role and it sounds like you have experience with the \"Ops\" side. Write in your SOP that you hope to leverage this degree into a seamless transition from infra to MLops and you should be fine.  .....Also write about learning GPU programming to write high performant and large scale machine learning systems l\n\n...Write a conclusion here....\n\n\n<!-- In 2022, having achieved what I had been striving for - a high salary, deep expertise in backend engineering, and decent fitness levels, I realized that I still wasn't content and I started to look outside of work to find this feeling. All my life, I have been obsessed with music. Growing up, I spent years training in eastern classical music and my mother was also clasically trained and natually, I was surrounded by music at all times. Even through the college years, the one thing that was a constant in my life through all of the lows was - house/tech-house/techno music. On an average, I would listen to my favorite artists for at least 5-7 hours a day. I was obsessed with details in the tracks, how the hi-hats sounded so polished, how th snare fills were disperesed through the tracks, how strings were use to build tension, how fx helped with transitions and how dissonant chords and stabs sounded so in-key with the track. I started to make house music and I have attached a few of my favorite tracks that I don't hate(I struggle with liking my own music) in my portfolio. As I made more music, I started to feel more content and more at peace. I slowly started to realize that this is one of the things that brings me true joy, pleasure and peace. I'm not for a minute suggesting that every minute of making music is blisseverything is fun till it's just a hobby, in fact since 2024 when I started taking it more seriously, making music is just like any other task, you have your highs and your lows. Some days, that feeling of finally finding that one vocal sample that bring your track together or that one leading note in your chord progression that makes the bridge is very intoxicating, but there there are days where hours go by and nothing of substance comes out, twiddling with frequencies and notes for hours at end and a 4min elevator music track to show for it. But despite all of this, making music brings me great joy and pleasure something that is unparalleled and something that I have never felt with hours of coding and building some very complex systems.\n\nAlright, now that I have set the stage and context I think it's time to convey why Architectre and how the above fits into this brief. I grew up in a beautiful apartment in Mumbai and my mother worked with an architect to build our apartment. Only now do I realize that it was through those years that I might have developed an eye for design, aesthetics and space (please replace with more architecture friendly words). EVen growing up, I was obsessed with finding websites that had good UX and I would configure my desktop and phone to take advantage of these beautiful apps so my digitial life was intuitively organized. This started to bleed into my personal projects as well and it can be seen in a tool that I developed years ago to consolidate all of my music from different sources like Spotify, Discogs, Bandcamp, Youtub and Soundcloud. In 2023 after buying my apartment, I decided to do the interiors myself. I first purchased a book called Studio Craft and Technique for Architects and I taught myself how to do perspective drawings, I have attached one in my portfolio. I then watched a few videos on how to use AutoCAD and modeled the entire upper floor of the apartment. I went through a few different iterations really trying to think through what the primary function of that space would be and I was able to problem solve my way through a plan for that space. Upon execution, that space which is my primary office, was a big success. I then translated that approach to the rest of the apartment, including the bedroom and the living room, details of which I have included in my portfolio. But as I worked through different iterations of the floor plans and the models, I realized that I was falling in love with this process. In the little experience that I have had with this work, I found the problem solving part of it to be extremely stimulating. It is similar to programming in that sense that you have various tools at your disposal to \"solve\" the problem, solving the problem is usually all about trade offs, about finding the right balance between form and function; similar to programming where you can optimize on speed or memory. I also found that unlike a lot of other topics that I have learned over the years, this came very naturally to me. I have since started going through the works of George Luss and Dieter Rams.....(fill in examples of work they have done here please) and in my free time, tend to watch videos about space, structures ... fill in some more architecture related stuff here showing that I am obsessed with it.\n\nAround two years ago, I decided for myself that by 2025 I would have made at least 3 professional sounding tracks that I am proud of. This might sound like a benign goal but considering my refined taste in music and the fact that I am extremely harsh on my work, it was a tenable goal that I had to work really hard towards achieving. Two months ago, I decided that I want to learn the skills to be an architect. I want to be able to build structures, homes, entities. I have no financial motive behind this decision. I don't publish any of my music online or on social media. I make music for myself(and my wife) and because I love doing it. Not to say that I am opposed to playing my music to others, but that's not my primary goal. Similarly, I simply want to learn how to build structures and doing an online Masters at ASU is one of the very few ways working professionals like me can learn this skill. In theory, I could device a plan to get to where I want to without doing an MArch; but having a structured program to follow with guidance from the faculty at ASU will accelerate this process. What I have tried to highlight through the above paragraphs in the SOP is that 1. I have the courage and determination to drag myself out of adversity 2. I have the ability to learn from my failures and come out stronger 3. I have demonstarted the ability to pick up a new skill in music production much later in life and hone it to a semi-professional level 4. I am driven by my passions and I can go to any length to fulfill them. At 32, I feel like it's now or never for me to do this MArch degree and I would be remiss to not give it a real shot.\n\n.... Please also include some notes about ASU in the SOP and mention that fact that I will be doing this degree part time while working full time and that my wife is on board with this.... -->","n":0.02}}},{"i":26,"$":{"0":{"v":"TO-DOs","n":1},"1":{"v":"- [ ] State ID \n- [ ] Bedroom\n    - [ ] Order another IKEA closet for roo downstairs\n    - [ ] Order panel lighting\n    - [ ] Change back curtains\n- [ ] Clean up the terrace, get rid\n- [ ] Fix the wardrobe\n- [ ] Install record player\n- [ ] Clean up basement\n- [ ] Pimp out the balcony\n\n### [[ML]]\n\n- [ ] Watch the rest of the 3Blue1Brown Videos\n- [ ] Make notes on the 3Blue1Brown Videos\n- [ ] Review the entier ML specialization and make notes\n- [ ] Finish reading the AI Engineering Book\n- [ ] Sync highlights from the book into Dendron","n":0.098}}},{"i":27,"$":{"0":{"v":"Goals","n":1},"1":{"v":"- [[Statistics]]\n    - OpenStax Intro to Statistics - _In-Progress_\n\n- [[Knowledge Base]]\n    - Dendron A Day video: https://www.youtube.com/watch?v=1mXGyG9ikD4\n\n- [[Math]]\n    - [[Math.Pre-Calculus]]\n    - Calc 1\n    - Calc 2\n    - Calc 3\n    - Linear Algebra\n    - Differential Equations\n\n- [[Physics]]\n    - [[Physics.OpenStax High School Physics]] - _In-Progress_\n    - Physics 2\n    - Physics 3\n    - Physics 4\n    - ECE\n\n- [[Design]]\n\n- [[ML]] \n    - [[ML.3Blue1Brown]]\n    - [[ML.AK]]\n    - [[ML.Scrimba AI Engineering Path]]\n\n- [[Carpentry]]\n\n- [[Music Production]]\n    - [[Music Production.Christopher Ledger]]\n    - [[Ned Rush Videos]]\n\n- [[DJ]]\n    - [[DJ.How to DJ Properly]]\n\n- [[System Design]]\n    - [[System Design.Database Engineering Course]]\n    - [[System Desing.Node JS Internals]]\n    - [[System Design.Node JS Paid Course]]\n    - [[System Design.Book]]\n\n- [[Security]]\n\n- [[Infra]]\n    - Docker\n    - Kubernetes\n    - Vercel\n    - Neon Postgres\n\n- [[Web Development]]\n    - [[Next JS.Tutorial]] - _In-Progress_\n        - There's a lot more to learn about these topics, including optimizing remote images and using local font files. If you'd like to dive deeper into fonts and images, see:\n            - [Image Optimization Docs](https://nextjs.org/docs/app/building-your-application/optimizing/images)\n            - [Font Optimization Docs](https://nextjs.org/docs/app/building-your-application/optimizing/fonts)\n            - [Improving Web Performance with Images (MDN)](https://developer.mozilla.org/en-US/docs/Learn/Performance/Multimedia)\n            - [Web Fonts (MDN)](https://developer.mozilla.org/en-US/docs/Learn/CSS/Styling_text/Web_fonts)\n            - [How Core Web Vitals Affect SEO](https://vercel.com/blog/how-core-web-vitals-affect-seo)\n            - [How Google handles JavaScript throughout the indexing process](https://vercel.com/blog/how-google-handles-javascript-throughout-the-indexing-process)\n            - [Partial Rendering](https://www.youtube.com/watch?v=MTcPrTIBkpA&ab_channel=Delba)\n    - [[Javascript Info]]\n    - [[React Tutorial]]\n    - [[Next JS.Docs]] - https://nextjs.org/docs/app/building-your-application/deploying\n    - [[Scrimba Front End Path]]\n    - [[CSS course]]\n        - Tailwind Cheat Sheet - https://flowbite.com/tools/tailwind-cheat-sheet/\n        - https://www.udemy.com/user/jonasschmedtmann","n":0.068}}},{"i":28,"$":{"0":{"v":"DOs","n":1},"1":{"v":"\n- [X] Unpack 📦\n- [X] Taxes 💰\n- [X] Roshani lawyer work ⚖️\n- [X] Setup upstairs room short term 🏠\n- [X] Order groceries 🛒:\n    - [X] drinking milk\n    - [X] ginger\n    - [X] green chillies\n    - [X] tomatoes\n    - [X] curry leaves\n    - [X] yogurt\n    - [X] mung\n    - [X] moth\n    - [X] tilapia\n    - [X] lemon\n    - [X] fruits\n    - [X] mustard leaves\n    - [X] carrots\n    - [X] green pepper\n    - [X] cilantro\n- [X] Order 🛍️:\n    - [X] Dimmer switches\n    - [X] Tushy\n    - [X] Futon\n    - [X] Zara Items\n- [X] Get milk for paneer 🍼\n- [X] Deposit money 💵\n- [X] Handle Affirm payments 💳\n- [ ] Schedule 📅:\n    - [X] Electrician\n    - [X] Plumber\n- [X] Ordern night gown for Roshani\n- [ ] Book Zurich trip\n- [X] Order colorful standard t-shirt\n","n":0.087}}},{"i":29,"$":{"0":{"v":"SP","n":1}}},{"i":30,"$":{"0":{"v":"USP","n":1}}},{"i":31,"$":{"0":{"v":"Vendor List Additions","n":0.577},"1":{"v":"\nComment: All of the user consent updates happen through the `/consent-status` endpoint.\n\n### Add a new vendor\nWe pull in all of the categories that the user has consented to and populate all of the vendors for those categories.\n\n### Add a new Opt-Out(DNS or Sensitive Data) IAB or Custom category\n- We opt the user in to *any* of these categories.\n\n### Add a new Opt-In IAB or Custom Category\n- We don't need to update the user's consent\n- _Requires Reconsent_?\n\n### Reconsent flow\n- Returning user has a consent record with dateCreated - `2025-03-01`\n- Opt-Out IAB Category, Opt-In IAB Category, Opt-In Custom Category were added with additionsChangeDate - `2025-03-04`\n\n#### Date Created holistic or / Category\n- This is what we do for GDPR. *Not* an option because consent record needs to be updated if an opt-out category is added which will cause the reconsent flow to break after page refresh.\n\n#### Shown Categories\n- Diff between shown opt-in categories and vendor list categories and pass down `reconsent` flag.\n\n#### Rejected Categories\n- Store rejected categories \n- Not backwards compatible\n","n":0.077}}},{"i":32,"$":{"0":{"v":"Unified UUID","n":0.707},"1":{"v":"# Unifying UUIDs across legislations and preferences\n\n## Client does GDPR + Preferences\n\n### Use cases to consider:\n\n#### Auth consent\n\n#### Non-auth consent\n\n#### Message chaining\n\n#### Expiration of consent\n\n#### Auth mapping and updating that after consent expiration\n\n#### New Client\n\n#### Existing client with preferences add-on","n":0.158}}},{"i":33,"$":{"0":{"v":"Preferences","n":1},"1":{"v":"\n### To-Dos\n- Implement caching with Redis\n- Check every route for race conditions in the validations\n- Implement transactions for routes like the download controller","n":0.209}}},{"i":34,"$":{"0":{"v":"Legal Doc Publish Changes","n":0.5},"1":{"v":"\n### Schema changes\n\n- Add `status` - `'LIVE' | 'PREV_LIVE' | 'DRAFT'`\n- Add `publishedAt` - `Date`\n- Remove goLiveDate\n\n### API Changes\n\n- Change the query for fetching the latest doc for a container by leveraging the `status` instead of the `goLiveDate`.\n\n- Update ALL Legal Doc APIs to conform with new schema.\n\n- Update relevant Legal Doc APIs to account for new features - \n    - New PUT `/publish` endpoint that publishes a new legal doc:\n        - Only a doc in `DRAFT` mode can be published.\n        - Need to mark currently `LIVE` doc as `PREV_LIVE`.\n        - Need to update `publishedAt` date.\n    - POST endpoint should set the status of the legal doc to `DRAFT`.\n\n","n":0.096}}},{"i":35,"$":{"0":{"v":"KV Targeting","n":0.707},"1":{"v":"\n- We are returning `status` and `rejectedStatus` and each category has a `dateConsented`.\n- The `/latest-version` endpoint returns each category and the `publishedAt` date for it.\n- The script then iterates through rejected status and it builds the `rejected kv targets`.\n- The script goes through status and checks to see if consent is valid if it is, it builds it as a `consented kv target` or a `outdated kv target`.\n-  ","n":0.121}}},{"i":36,"$":{"0":{"v":"Auth Consent","n":0.707},"1":{"v":"\n# Implementing auth consent for preferences\n\n## /GET Consent status flow\n- if valid `authId` present\n    - if `uuidGenerated`\n        - get `authMapping(uuid)` by `authId`\n        - set the preference `uuid` value in local-storage.\n            - If no authMapping\n                - we don't neede to call get user consent.\n            - Else if authMapping \n                - get the user consent for this  `uuid` since it might have changed on another device.\n    - else if a `uuid` was passed and it *wasn't* generated\n        - get `authMapping(uuid)` by `authId`\n            - if *no* `authMapping` exists for this `authId`, \n                - then *create* the `authMapping` record to store this new mapping between the `authId` and the `uuid`.\n                - get the user consent for this `uuid` since it might have changed on some other device.\n            - else if `authMapping` exists \n                - if `linkedUuid` is the same\n                    - if `localData` is current then short circuit\n                    - else if `localData` is *not* current. _Does this happen through the VL?_\n                        - get the user consent for this `uuid` since it might have changed on some other device.\n                - else if `linkedUuid` is *not* the same\n                    - get the user consent for this new `uuid` since it might have changed on another device. We use the linked UUID to fetch the consent data since that is the latest uuid that might have newer changes.\n                    - set the preference `uuid` value in local-storage\n- else\n    - Nothing to do.\n\n\n## /POST preference flow\n- if valid `authId` and `uuid` present, then create `user preference` and `authMapping`.O\n\n## Open Questions\n- Why are we checking to see if the uuids are the same for the short circuit?","n":0.061}}},{"i":37,"$":{"0":{"v":"Message Renderer","n":0.707},"1":{"v":"## Design\n\n### SP.Message Renderer.Step 1- _1.5 weeks_\n\nThe general idea here would be to create a RAG based system, where in we store the `messageJSON` along with some `metadata`(description of the messageJSON) in a vector database and then test the validity of the retrival and generation capabilites of the LLM.\n\nThis step won't have an interactive experience like that of an assistant. It will *only* have the core functionality of outputting a `msgJSON` given a prompt. It will be a CLI tool that one can run locally.\n\n### Step 2 - _1 week_\n\nDuring this week we will build out an evaluation systems where we will take 20 msgJSONs from the system that we create ourselves using a `prompt` and then we will provide the same prompt to the LLM and test the validity of the results. \n\n> Based on what we find, we will increase the number of examples of msgJSONs our vector database or pivot to a different approach like `fineTuning`. If we find that we might need to rethink the structure of the `msgJSON` to better leverage the LLMs abilities to _only_ alter the **style** and **text** of the message, we might have to re-think the `messageJSON` structure and change the way the rendering application works to accommodate for this change in structure - **PIVOT CAN TAKE UPTO 3 WEEKS**\n\n### Step 3 - _2 weeks_\n\nAssuming that the RAG approach works, we will then build out an assistant type experience to generate the `msgJSON`. This _may_ entail building out a service that can maintain conversation context / state so multiple clients can interact with it. We _may_ implement streaming responses using OpenAI's streaming API.\n\n### Step 4 - _? weeks_\n\nWe will then build out the front-end integrations within portal to integrate with this message rendering service. The integration will be a chat like experience where in you can have a conversation with an \"agent\" to build you a message. \n\n### Setting expectations\n\n- What percentage of customer messages do you want the AI renderer to automate?\n\n- How much quicker are customers able to setup messages?\n\n- How much human labor does the chatbot save?\n\n- What is the customer sentiment and feedback for this system?\n\n\n### Milestone planning\n\n- End customer review of message quality\n\n- Cost: how much it costs / inference\n\n### Maintenance\n\n","n":0.052}}},{"i":38,"$":{"0":{"v":"Step 1","n":0.707},"1":{"v":"\n### AWS Updates\n\nWe are going to have store the `metadata` in Knowledge Base. The `metadata` is also going to include the s3 url. We will use an LLM to fetch the appropriate metadata, based on user prompt and then use the url and make changes to the template in memory.\n\n### Other Updates\n\nIt seems like Claude does a very good job with manipulting the msgJSON based on prompts. An alternative solution could be to use pinecone as the vector database and Anthropic's API to do the prompt rendering and computation.","n":0.106}}},{"i":39,"$":{"0":{"v":"Contributions","n":1},"1":{"v":"# Professional Contributions and Value Summary\n\n## Leadership & Critical Role\n\n- **Acting Product Lead for Preferences** - Providing continuity and leadership despite turnover in the product team\n- **Critical Issue Response Lead** - First responder for all critical and security issues, ensuring minimal downtime and business impact\n- **Product Development Leadership** - Leading development of mission-critical products including:\n  - Preference Service\n  - Preference Product\n  - Legal Document Manager\n  - Mailchimp, Hubspot & Braze integrations with propagation sync and imports.\n- **Cross-functional Leadership** - Maintaining team morale and managing stakeholder expectations during high-pressure situations\n\n## Technical Contributions & Innovations\n\n- **Core Infrastructure Development**\n  - Designed and built authentication and authorization service in-house, saving approximately $36,000 annually by eliminating the need for Auth0\n  - Led cost optimization initiative, significantly reducing operational expenses\n  - Built and maintained core backend services with comprehensive testing and deployment\n  \n- **Privacy Compliance Solutions**\n  - Architected and implemented fast JavaScript solutions for:\n    - GDPR compliance\n    - CCPA compliance\n    - Unified legislation framework\n  - Developed wrapper API to streamline implementation across all client platforms\n\n- **Product Development**\n  - Built US privacy product from concept to launch, including stakeholder management\n  - Designed and implemented Preferences system, overcoming significant technical obstacles\n\n## Team Support & Enablement\n\n- **Technical Guidance**\n  - Support Chaitanya, Tom, and Chris with documentation, training, testing, and POCs\n  - Collaborate with Dan on architecture decisions and implementations\n  - Manage and coordinate with Cybage Front End team\n\n- **Cross-team Support**\n  - Assist with security questionnaires and ISO compliance\n  - Support M&A technical assessment activities\n  - Provide critical assistance with Portal issues\n  - Help onboard and mentor new team members (e.g., Marina)\n\n## Exceptional Commitment\n\n- **Consistent Availability** - Provide 24/7 support for critical business needs, including during personal time\n- **Above-and-Beyond Performance** - Successfully completed major deliverables under challenging circumstances, including:\n  - Built preferences system while undergoing multiple dental procedures during international travel\n  - Completed cost optimization project during personal leave for family commitments\n- **Versatility** - Adapt to fill gaps across product, UX, and integration needs without requiring direction\n\n## Value Added to Business\n\n- **Cost Savings** - In-house development of services that would otherwise require expensive third-party solutions\n- **Risk Mitigation** - First-response capabilities for security and critical issues\n- **Product Continuity** - Filling product leadership gaps to ensure consistent progress\n- **Tech","n":0.052}}},{"i":40,"$":{"0":{"v":"Roshani","n":1}}},{"i":41,"$":{"0":{"v":"Workout Regime","n":0.707},"1":{"v":"\n# Workout I\n\n> Machine based workout for hypertrophy and toning.\n\n### Lying Leg Curls\n\n- [ ] Warm-Up Rep 1 - ___ (ideally 7 v slow reps)\n- [ ] Warm-Up Rep 2 - ___ (ideally 7 v slow reps)\n- [ ] 1 x 13 - ___\n- [ ] 1 x 13 - ___\n- [ ] 1 x 13 - ___\n\n### Leg Press\n\n- [ ] Warm-Up Rep 1 - ___ (ideally 4 v slow reps)\n- [ ] Warm-Up Rep 1 - ___ (ideally 4 v slow reps)\n- [ ] 1 x 7 - ___\n- [ ] 1 x 7 - ___\n- [ ] 1 x 7 - ___\n\n### Seated Leg Curls\n\n- [ ] Warm-Up Rep 1 - ___ (ideally 4 v slow reps)\n- [ ] Warm-Up Rep 1 - ___ (ideally 4 v slow reps)\n- [ ] 1 x 10 - ___\n- [ ] 1 x 10 - ___\n- [ ] 1 x 10 - ___\n","n":0.081}}},{"i":42,"$":{"0":{"v":"Next JS","n":0.707}}},{"i":43,"$":{"0":{"v":"Tutorial","n":1},"1":{"v":"\n\n### Intro \n\n- The DOM is an object representation of the HTML elements. It acts as a bridge between your code and the user interface, and has a tree-like structure with parent and child relationships.\n\n- You can use DOM methods and JavaScript, to listen to user events and manipulate the DOM by selecting, adding, updating, and deleting specific elements in the user interface. DOM manipulation allows you to not only target specific elements, but also change their style and content.\n\n- Updating the DOM with plain JavaScript is very powerful but verbose. You've written all this code to add an `<h1>` element with some text:\n\n- As the size of an app or team grows, it can become increasingly challenging to build applications this way - writing code that manuipulates the DOM to perform business logic.\n\n- With this approach, developers spend a lot of time writing instructions to tell the computer how it should do things. But wouldn't it be nice to describe what you want to show and let the computer figure out how to update the DOM?\n\n### Trees in React\n\n- Trees are a common way to represent the relationship between entities. They are often used to model UI.\n\n- Render trees represent the nested relationship between React components across a single render.\n\n- With conditional rendering, the render tree may change across different renders. With different prop values, components may render different children components.\n\n- Render trees help identify what the top-level and leaf components are. Top-level components affect the rendering performance of all components beneath them and leaf components are often re-rendered frequently. Identifying them is useful for understanding and debugging rendering performance.\n\n- Dependency trees represent the module dependencies in a React app.\n\n- Dependency trees are used by build tools to bundle the necessary code to ship an app.\n\n- Dependency trees are useful for debugging large bundle sizes that slow time to paint and expose opportunities for optimizing what code is bundled.Trees are a common way to represent the relationship between entities. They are often used to model UI.\n\n\n- Render trees represent the nested relationship between React components across a single render.\n\n- With conditional rendering, the render tree may change across different renders. With different prop values, components may render different children components.\n\n- Render trees help identify what the top-level and leaf components are. Top-level components affect the rendering performance of all components beneath them and leaf components are often re-rendered frequently. Identifying them is useful for understanding and debugging rendering performance.\n\n- Dependency trees represent the module dependencies in a React app.\n\n- Dependency trees are used by build tools to bundle the necessary code to ship an app.\n\n- Dependency trees are useful for debugging large bundle sizes that slow time to paint and expose opportunities for optimizing what code is bundled.\n\n### Advantages of React\n\n- **Keeping a button’s rendering logic and markup together ensures that they stay in sync with each other on every edit. Conversely, details that are unrelated, such as the button’s markup and a sidebar’s markup, are isolated from each other, making it safer to change either of them on their own.** - this is one of the main advantages of React, along with... \n\n- **The main performance advantage comes from React's Virtual DOM approach: In vanilla JS: When you add a new tag, you'd typically: Get a reference to the dropdown, Create a new element, Set its properties, Append it to the DOM, This triggers a complete recalculation of layout, styles, and page rendering (reflow and repaint) which is expensive, especially if your dropdown has many items. In React: When a tag is added: React updates its virtual DOM first (which is just a JavaScript object), Compares it with the previous state (diffing), Only applies the minimal necessary changes to the real DOM.**\n\n- Components allow you to build self-contained, reusable snippets of code. If you think of components as LEGO bricks, you can take these individual bricks and combine them together to form larger structures. If you need to update a piece of the UI, you can update the specific component or brick.\n\n- This modularity allows your code to be more maintainable as it grows because you can add, update, and delete components without touching the rest of our application.\n\n### Working with JSX\n\n- To return multiple elements from a component, wrap them with a single parent tag.\n\n- For example, you can use a `<div>`:\n\n- If you don’t want to add an extra `<div>` to your markup, you can write <> and </> instead:\n\n- This empty tag is called a **Fragment**. Fragments let you group things without leaving any trace in the browser HTML tree.\n\n- JSX looks like HTML, but under the hood it is transformed into plain _JavaScript objects_. You can’t return two objects from a function without wrapping them into an array. This explains why you also can’t return two JSX tags without wrapping them into another tag or a Fragment.\n\n- But browsers don't understand JSX out of the box, so you'll need a JavaScript compiler, such as a Babel, to transform your JSX code into regular JavaScript.\n\n### Passing data with props\n\n- Regular HTML elements have attributes that you can use to pass pieces of information that change the behavior of those elements. For example, changing the src attribute of an ``<img>`` element changes the image that is shown. Changing the href attribute of an `<a>` tag changes the destination of the link.\n\n- In the same way, you can pass pieces of information as properties to React components. These are called props. Take for instance, the possible variations of a button:\n\n- Note: In React, data flows down the component tree. This is referred to as one-way data flow. State, which will be discussed in the next chapter, can be passed from parent to child components as props.\n\n- You can think of curly braces as a way to enter \"JavaScript land\" while you are in \"JSX land\". You can add any JavaScript expression (something that evaluates to a single value) inside curly braces.\n\n### Managing state with hooks\n\n- React has a set of functions called hooks. Hooks allow you to add additional logic such as state to your components. You can think of state as any information in your UI that changes over time, usually triggered by user interaction.\n\n- Sharing state between components \n\n- Sometimes, you want the state of two components to always change together. To do it, remove state from both of them, move it to their closest common parent, and then pass it down to them via props. This is known as “lifting state up”, and it’s one of the most common things you will do writing React code.\n\n- You can use Context and Reducer to share state between Parent and deep child components.\n\n\n### Responding to events \n\n- You can handle events by passing a function as a prop to an element like `<button>`.\n\n- Event handlers must be passed, not called! onClick={handleClick}, not onClick={handleClick()}.\n\n- You can define an event handler function separately or inline.\n\n- Event handlers are defined inside a component, so they can access props.\n\n- You can declare an event handler in a parent and pass it as a prop to a child.\n\n- You can define your own event handler props with application-specific names.\n\n- Events propagate upwards. Call e.stopPropagation() on the first argument to prevent that.\n\n- Events may have unwanted default browser behavior. Call `e.preventDefault()` to prevent that.\n\n- Explicitly calling an event handler prop from a child handler is a good alternative to propagation.\n\n### Server and Client Components\n \n- The Network Boundary is a conceptual line that separates the different environments.\n\n- In React, you choose where to place the network boundary in your component tree. For example, you can fetch data and render a user's posts on the server (using Server Components), then render the interactive LikeButton for each post on the client (using Client Components).\n\n- Similarly, you can create a Nav component that is rendered on the server and shared across pages, but if you want to show an active state for links, you can render the list of Links on the client.\n\n### Optimizing fonts and images\n\n- Fonts play a significant role in the design of a website, but using custom fonts in your project can affect performance if the font files need to be fetched and loaded.\n\n- **Cumulative Layout Shift(CLS)** is a metric used by Google to evaluate the performance and user experience of a website. With fonts, layout shift happens when the browser initially renders text in a fallback or system font and then swaps it out for a custom font once it has loaded. This swap can cause the text size, spacing, or layout to change, shifting elements around it.\n\n- Next.js automatically optimizes fonts in the application when you use the next/font module. It downloads font files at build time and hosts them with your other static assets. This means when a user visits your application, there are no additional network requests for fonts which would impact performance.\n\n- However, this means you have to manually:\n    - Ensure your image is responsive on different screen sizes.\n    - Specify image sizes for different devices.\n    - Prevent layout shift as the images load.\n    - Lazy load images that are outside the user's viewport.\n    - Image Optimization is a large topic in web development that could be considered a specialization in itself. Instead of manually implementing these optimizations, you can use the next/image component to automatically optimize your images.\n\n- By adding Inter to the `<body>` element, the font will be applied throughout your application. Here, you're also adding the Tailwind antialiased class _which smooths out the font. It's not necessary to use this class, but it adds a nice touch._\nNext.js can serve static assets, like images, under the top-level /public folder. Files inside /public can be referenced in your application.\n\n### Using Layouts\n\n- Dashboards have some sort of navigation that is shared across multiple pages. In Next.js, you can use a special layout.tsx file to create UI that is shared between multiple pages. Let's create a layout for the dashboard pages!\n\n- First, you're importing the `<SideNav />` component into your layout. Any components you import into this file will be part of the layout.\n\n- The `<Layout />` component receives a children prop. This child can either be a page or another layout.\n\n- One benefit of using layouts in Next.js is that on navigation, only the page components update while the layout won't re-render. This is called partial rendering which preserves client-side React state in the layout when transitioning between pages.\n\n- This is called a root layout and is required in every Next.js application. Any UI you add to the root layout will be shared across all pages in your application. You can use the root layout to modify your `<html>` and `<body>` tags, and add metadata (you'll learn more about metadata in a later chapter).\n\n### Routing and code-splitting\n\n- Code splitting allows you to split your application code into smaller bundles to be downloaded and executed by the browser. This reduces the amount of data transferred and execution time for each request, leading to improved performance.\n\n- Server Components allow your application code to be automatically code-split by route segments. This means only the code needed for the current route is loaded on navigation.\n\n\n- The App Router uses a hybrid approach for routing and navigation. On the server, your application code is automatically code-split by route segments. And on the client, Next.js prefetches and caches the route segments. This means, when a user navigates to a new route, the browser doesn't reload the page, and only the route segments that change re-render - improving the navigation experience and performance.\n\n\n- To improve the navigation experience, Next.js automatically code splits your application by route segments. This is different from a traditional React SPA, where the browser loads all your application code on the initial page load.\n\n- Splitting code by routes means that pages become isolated. If a certain page throws an error, the rest of the application will still work. This is also less code for the browser to parse, which makes your application faster.\n\n- Furthermore, in production, whenever `<Link>` components appear in the browser's viewport, Next.js automatically prefetches the code for the linked route in the background. By the time the user clicks the link, the code for the destination page will already be loaded in the background, and this is what makes the page transition near-instant!\n\n\n### Fetching Data \n\nAPI layer\n- APIs are an intermediary layer between your application code and database. There are a few cases where you might use an API:\n    - If you're using third-party services that provide an API.\n    - If you're fetching data from the client, you want to have an API layer that runs on the server to avoid exposing your database secrets to the client.\n\n- In Next.js, you can create API endpoints using Route Handlers.\n\n- Database queries\n    - When you're creating a full-stack application, you'll also need to write logic to interact with your database. For relational databases like Postgres, you can do this with SQL or with an ORM.\n\n- There are a few cases where you have to write database queries:\n    - When creating your API endpoints, you need to write logic to interact with your database.\n    - If you are using React Server Components (fetching data on the server), you can skip the API layer, and query your database directly without risking exposing your database secrets to the client.\n\n- Using Server Components to fetch data. By default, Next.js applications use React Server Components. Fetching data with Server Components is a relatively new approach and there are a few benefits of using them:\n\n- Server Components support JavaScript Promises, providing a solution for asynchronous tasks like data fetching natively. You can use async/await syntax without needing useEffect, useState or other data fetching libraries.\n\n- Server Components run on the server, so you can keep expensive data fetches and logic on the server, only sending the result to the client.\n\n- Since Server Components run on the server, you can query the database directly without an additional API layer. This saves you from writing and maintaining additional code.\n\n### Project structure for a Next JS app\n\n- `/app`: Contains all the routes, components, and logic for your application, this is where you'll be mostly working from.\n\n- `/app/lib`: Contains functions used in your application, such as reusable utility functions and data fetching functions.\n\n- `/app/ui`: Contains all the UI components for your application, such as cards, tables, and forms. To save time, we've pre-styled these components for you.\n\n- `/public`: Contains all the static assets for your application, such as images.\n\n- Config Files: You'll also notice config files such as next.config.ts at the root of your application. Most of these files are created and pre-configured when you start a new project using create-next-app. You will not need to modify them in this course.\n\n### Working with images in Next JS\n\n- Next.js can serve static assets, like images, under the top-level /public folder. Files inside /public can be referenced in your application.\n\n- With regular HTML, you would add an image as follows:\n    ```\n    <img\n    src=\"/hero.png\"\n    alt=\"Screenshots of the dashboard project showing desktop version\"\n    />\n    ```\n\n- However, this means you have to manually:\n    - Ensure your image is responsive on different screen sizes.\n    - Specify image sizes for different devices.\n    - Prevent layout shift as the images load.\n    - Lazy load images that are outside the user's viewport.\n    - Image Optimization is a large topic in web development that could be considered a specialization in itself. Instead of manually implementing these optimizations, you can use the next/image component to automatically optimize your images.\n\n- The `<Image>` Component is an extension of the HTML `<img>` tag, and comes with automatic image optimization, such as:\n    - Preventing layout shift automatically when images are loading.\n    - Resizing images to avoid shipping large images to devices with a smaller viewport.\n    - Lazy loading images by default (images load as they enter the viewport).\n    - Serving images in modern formats, like WebP and AVIF, when the browser supports it.\n\n- It's good practice to set the width and height of your images to avoid layout shift, these should be an aspect ratio *identical* to the source image. These values are not the size the image is rendered, but instead the size of the actual image file used to understand the aspect ratio.\n\n### Static Rendering vs Dynamic Rendering\n\n- With **static rendering**, data fetching and rendering happens on the server at build time (when you deploy) or when revalidating data. Whenever a user visits your application, the cached result is served.\n\n- There are a couple of benefits of static rendering:\n    - Faster Websites - Prerendered content can be cached and globally distributed when deployed to platforms like Vercel. This ensures that users around the world can access your website's content more quickly and reliably.\n    - Reduced Server Load - Because the content is cached, your server does not have to dynamically generate content for each user request. This can reduce compute costs.\n    - SEO - Prerendered content is easier for search engine crawlers to index, as the content is already available when the page loads. This can lead to improved search engine rankings.\n    - Static rendering is useful for UI with no data or data that is shared across users, such as a static blog post or a product page. It might not be a good fit for a dashboard that has personalized data which is regularly updated.\n\n- With **dynamic rendering**, content is rendered on the server for each user at request time (when the user visits the page). There are a couple of benefits of dynamic rendering:\n\n    - Real-Time Data - Dynamic rendering allows your application to display real-time or frequently updated data. This is ideal for applications where data changes often.\n    - User-Specific Content - It's easier to serve personalized content, such as dashboards or user profiles, and update the data based on user interaction.\n    - Request Time Information - Dynamic rendering allows you to access information that can only be known at request time, such as cookies or the URL search parameters.\n \n### Streaming and Suspense\n\n- Deciding where to place your Suspense boundaries:\n    - You could stream the whole page like we did with loading.tsx... but that may lead to a longer loading time if one of the components has a slow data fetch.\n    - You could stream every component individually... but that may lead to UI popping into the screen as it becomes ready.\n    - You could also create a staggered effect by streaming page sections. But you'll need to create wrapper components.\n    - Where you place your suspense boundaries will vary depending on your application. In general, it's good practice to move your data fetches down to the components that need it, and then wrap those components in Suspense. But there is nothing wrong with streaming the sections or the whole page if that's what your application needs.\n\n### Search and Pagination\n\n- As mentioned above, you'll be using URL search params to manage the search state. This pattern may be new if you're used to doing it with client side state.\n\n- There are a couple of benefits of implementing search with URL params:\n    - **Bookmarkable and shareable URLs**: Since the search parameters are in the URL, users can bookmark the current state of the application, including their search queries and filters, for future reference or sharing.\n    - **Server-side rendering**: URL parameters can be directly consumed on the server to render the initial state, making it easier to handle server rendering.\n    - **Analytics and tracking**: Having search queries and filters directly in the URL makes it easier to track user behavior without requiring additional client-side logic.\n\n- When to use the useSearchParams() hook vs. the searchParams prop?\n    - You might have noticed you used two different ways to extract search params. Whether you use one or the other depends on whether you're working on the client or the server.\n    - `Search` is a Client Component, so you used the useSearchParams() hook to access the params from the client.\n    - `Table` is a Server Component that fetches its own data, so you can pass the searchParams prop from the page to the component.\n    - As a general rule, if you want to read the params from the client, use the useSearchParams() hook as this avoids having to go back to the server.\n\n- Revalidate and redirect\n    - Next.js has a client-side router cache that stores the route segments in the user's browser for a time. Along with prefetching, this cache ensures that users can quickly navigate between routes while reducing the number of requests made to the server.\n\n    - Since you're updating the data displayed in the invoices route, you want to clear this cache and trigger a new request to the server. You can do this with the revalidatePath function from Next.js.","n":0.017}}},{"i":44,"$":{"0":{"v":"Docs","n":1}}},{"i":45,"$":{"0":{"v":"Music Production","n":0.707}}},{"i":46,"$":{"0":{"v":"Christopher Ledger","n":0.707},"1":{"v":"\n## Video List : \n+ [ ] [[Music Production.Christopher Ledger.How to Sample Classic Breaks]] ","n":0.267}}},{"i":47,"$":{"0":{"v":"How to Sample Classic Breaks","n":0.447}}},{"i":48,"$":{"0":{"v":"Math","n":1}}},{"i":49,"$":{"0":{"v":"Pre-Calculus","n":1}}},{"i":50,"$":{"0":{"v":"Tutorial","n":1},"1":{"v":"\nWelcome to Dendron! Dendron is a developer-focused knowledge base that helps you manage information using **flexible hierarchies**!\n\nYou are currently in the tutorial vault (a vault is the folder where your notes are stored). Feel free to edit this note and create new files as you go through the quickstart!\n\n## Create a Note\n\n1. Use `Ctrl+L` / `Cmd+L` to bring up the lookup prompt\n1. Type `dendron` and select `Create New`\n\n- > NOTE: After you press enter, Dendron will create and open the `dendron` note. Use `<CTRL>-<TAB>` to come back to this note\n\nYou just created your first note!\n\n- > NOTE: Notes in Dendron are just plain text markdown with some [frontmatter](https://wiki.dendron.so/notes/ffec2853-c0e0-4165-a368-339db12c8e4b) on the top. You can edit them in Dendron or using ~~vim~~ your favourite text editor.\n\n## Find a Note\n\n1. Use `Ctrl+L` / `Cmd+L` to bring up the lookup prompt again\n1. Type `dendron` and press `<ENTER>`\n\n- > TIP: you don't have to type out the entire query, press `<TAB>` to autocomplete\n\nYou just `looked up` a note!\n\n- > NOTE: in Dendron, you can find or create notes using the lookup prompt\n\n## Organize your Notes\n\n1. Bring up the lookup prompt again\n1. Type `tutorial.one`\n\nYou just created your first hierarchy!\n\n- > NOTE: hierarchies in Dendron are just `.` delimited files. This makes each note both a file and a folder and makes it easy to keep your notes organized\n\n- > TIP: You can use the [Dendron Tree View](https://wiki.dendron.so/notes/hur7r6gr3kqa56s2vme986j) to view your hierarchy. If it's not currently in focus, you can use `CTRL+SHIFT+P`/`CMD+SHIFT+P` to open the command prompt and type in `Dendron: focus on tree view` to make it appear\n\n## Create a link\n\n1. In the current note, type `[[` - this should trigger the autocomplete. You can type `one` to narrow it down to the note you just created and hit enter\n<!-- Enter '[[' below-->\n\n<!-- End space-->\n\nYou just created your first link!\n\n- > NOTE: the links with the `[[` are called wikilinks (because they were first popularized by Wikipedia)\n- > TIP: If you hover your mouse over the link, you can get a preview of the contents inside the note!\n\n## Navigate a link\n\n1. Move your text cursor over the link you just created. Hold down `<CTRL>+<ENTER>`/`<CMD>+<ENTER>`\n\n- > TIP: You can also use `CTRL+CLICK` or `CMD+CLICK` to navigate links via mouse\n\nYou just navigated the link!\n\n## Refactor a Note\n\n1. Open [[tutorial.one]], bring up the command prompt (`CTRL+SHIFT+P`/`CMD+SHIFT+P`) and type `Dendron: Rename Note`\n1. Replace `tutorial` with `my-note` and then press `<ENTER>`\n1. You just refactored the note!\n\n- > NOTE: when you rename a note, Dendron updates all links and references of the original note being renamed. Try switching back to [[tutorial]] to see the updated link!\n- > TIP: in addition to renaming one note at a time, dendron has [an entire collection](https://wiki.dendron.so/notes/srajljj10V2dl19nCSFiC) of refactoring commands that let you change headers, move around sections, and refactor entire hierarchies!\n\n## Conclusion\n\nCongrats, you finished the Dendron tutorial!\n\nWas there anything **unclear or buggy** about this tutorial? Please [**report it**](https://github.com/dendronhq/dendron/discussions/3266) so we can **make it better**!\n\n## Next Steps\n\nDepending on your needs, here are some common next steps:\n\n- I want to **start writing**: [Create a daily journal note](command:dendron.createDailyJournalNote) ([docs](https://wiki.dendron.so/notes/ogIUqY5VDCJP28G3cAJhd))\n\n- I want to **use templates**: Use the [Appy Template](https://wiki.dendron.so/notes/ftohqknticu6bw4cfmzskq6) command to apply [templates](https://wiki.dendron.so/notes/861cbdf8-102e-4633-9933-1f3d74df53d2) to existing notes\n\n- I want to do a **longer tutorial**: Check out our [5min tutorial to explore more of Dendron's functionality](https://wiki.dendron.so/notes/678c77d9-ef2c-4537-97b5-64556d6337f1/)\n\n- I want to **implement a particular workflow** (bullet journal, zettelkasten, etc): Check out community [workflow guides](https://wiki.dendron.so/notes/9313b845-d9bf-42c9-aad1-0da34794ce26)\n\n- I want to use Dendron for **tasks and todos**: See the [Getting Things Done (GTD), Bullet Journaling, and Other Task Management Workflows](https://wiki.dendron.so/notes/ordz7r99w1v099v14hrwgnp) for how the founder of Dendron uses it to manage his work.\n\n- I want to explore **advanced features**: See [next steps](https://wiki.dendron.so/notes/TflY5kn29HOLpp1pWT9tP) for longer walkthroughs and advanced functionality!\n\n- I want to start clean with a **new vault at a custom location**: Run [Dendron: Initialize Workspace](command:dendron.initWS) from the command prompt (or click this link) to start from a clean slate\n\n- I want to use Dendron as a **knowledge base for my team**: Read the [Dendron team setup](https://wiki.dendron.so/notes/98f6d928-3f61-49fb-9c9e-70c27d25f838) to get started\n\n> Coming from Obsidian? Click [here](command:dendron.importObsidianPod) to import your Obsidian notes (or any markdown notes) into Dendron to see how they look.\n\n## Community\n\nDendron is more that just a tool - we are also a community of individuals that are passionate about knowledge management. If you need help or want to connect with the community, join us in the [Discords](https://link.dendron.so/discord).\n\nYou can also:\n\n- Star us on [GitHub](https://github.com/dendronhq/dendron)\n- Follow us on [Twitter](https://twitter.com/dendronhq)\n- Subscribe to the [Dendron Newsletter](https://link.dendron.so/newsletter)\n","n":0.037}}},{"i":51,"$":{"0":{"v":"ML","n":1},"1":{"v":"\nThis is where we will be tracking all things Machine Learning.\n\n# Coursera\n\n# Andrej Karpathy YT - [[ML.AK]]\n\n","n":0.243}}},{"i":52,"$":{"0":{"v":"AK","n":1},"1":{"v":"This is where we will be tracking all content released by Andrej Karpath\n\nIntro to LLMs for the general audience - [[ML.AK.Intro to LLMs]]\n\nDeep dive into LLMs like Chat GPT - ","n":0.183}}},{"i":53,"$":{"0":{"v":"Intro to LLMs","n":0.577},"1":{"v":"\n### Intro to LLMS\n\n- A LLM is just two files - a parameter file(140GB) and an executable(500 lines of code), eg: Llama-2-70B is the Lllama 2.0 model released by Meta which is trained on 70 B paramters. \n\n- The parameteres are essentially the weights or the paramaters of this neural network that is a LLM.\n\n- Llama is an open source; the weights, the architecture and the executable were released by Meta.\n\n- Chat GPT never released the model architecture so you don't have access to that model.\n\n- Model inference is when you just run the model on your macbook.\n\n- Model training is a lot more involved.\n\n- This LLAMA model is essentially just a next word prediction model. This might seem trivial, but being able to predic the next word(s) can be very powerful since if the objective is to be a next word predicion model, eg to be able to predict the next word about Ruth Handler, you need to parse a lot of information and you end up /learning/ a lot in the process. _All of this knowledge is being compressed into the weights of the neural network ??_. - The neural network isn't actually memorizing the data but instead storing complicated patterns and learning deeper patterns and each weight contributes to understanding multiple concepts.\n\n- First, about \"inference\" - this just means using the trained model to generate text, as opposed to training it. When you use ChatGPT or when Claude responds to you, that's inference.\n\n- The text generation process works like this:\n\n    - The model looks at the input text (like your question)\n\n    - It predicts what word is most likely to come next\n\n    - It adds that word to the sequence\n\n    - Then it uses the updated sequence to predict the next word\n\n    - This process repeats until it completes the response\n\n    - The phrase \"dreams internet documents\" is a colorful way of saying that the model recreates text patterns similar to its training data (which includes many internet documents). It's not literally dreaming or remembering specific documents - instead, it has learned patterns from millions of texts and can generate new text following similar patterns.\n\n### There are three main parts of building a model - **training**, **inference** and **fine-tuning**.\n\n    - Training involves firstly scraping the internet for loads of text, of the magnitude of 10TB. After scraping all of this data, the model is trained on this data and all of this data is then _compressed_ into the paramters and weights of the neural network. The LLAMA 2.0 model for instance has 70B parameters which is around 140GB of data. So you can think that the compression that takes place is some sort of lossy compression where all of the data is sort of compressed into these parameters. Training is the most expensive part of the process where a huge GPU cluster is leveraged for over 12 days(for LLAMA) and costs 2M dollars.\n\n    - Inference is the most straightforward part. When interacting with a model, the model performs inference and responds to your queries.\n\n    - Fine-Tuning is the step after inference where the model turns from being a next word prediction model to some sort of assistant.\n\n### How does it work?\n\n    - We don't know really. We know in full detail what the architecture of the neural network looks like and what math is performed at each step, but we still don't know _how_ these parameters/nodes which are dispersed through the neural network interact with each other.\n\n    - We know that they build some sort of knowledge base but it's a bit strange. We know that it works _emperically_, and we know how to fine-tune models and make them better based on evaluation tests.\n\n### Fine-Tuning\n\n    - We first come up with the labeling instructions for the fine tuning job, and then hire people or use scale.ai to perform these labeling tasks where they maybe compile 100,000 questions and answer type forms.\n\n    - These forms are very high quality.\n\n    - The model is then trained on these forms(training takes approx 1 day) and it then subscribes to the structure of these forms and turns in to an assistant.\n\n    - You can then run a lot of evalutaions on this assistant and go back to the first step again to imporove the model based on your evaluations.\n\n### Scaling Laws\n\n    - The performance of the model is governed by N - **number of parameters in the network** and `D` - **amount of text** it's trained on.\n\n    - So far, there is not upper bound on the performance so in theory we could just keep scaling.\n\n    - Algorithmic improvements would be a plus but in theory we can just keep getting better with more **time** and more **compute**.\n    \n### Tool use\n\n    - Tool use is another reason why these LLMs are becoming so capable. Being able to use tools like browsers, calculators, graphing libraries, image generation libraries etc empowers the LLMS to perform complex tasks like humans would.\n    ","n":0.035}}},{"i":54,"$":{"0":{"v":"Deep dive into Chat GPT","n":0.447},"1":{"v":"\n# Deep dive into LLMS like Chat GPT\n\n## Step 1 - Pretraining: download and pre-process the internet\n\n- This is the part where we scrape the internet for colossla amounts of quality text. This is something that **every** LLM company must have done. **Fineweb** is a dataset that is equivalent to the text dataset these companies assimmilate.\n\n- In order to collect all of this data, **Hugging Face**(the company that created Fineweb) started with using data from **Common Crawl** which has been crawling the internet since 2007. This data then goes through a _pipeline_ where we filter through various different items - like filter through websites on the blocklist, get rid of all of the HTML markup, get rid of text from items in the navigation panel on a website, so on and so forth.\n\n- At the end of this pipeline, you will have a clean high quality text data set.\n\n## Step 2 - Tokenization\n\n- Here we convert the sequence of text that was generated by concatenating the the text dataset from Step 1, into a sequence of _tokens_ or _symbols_. The reason we need to do this is because the neural network takes as input a one dimensional sequence of tokens or symbols and we want to optimize on the length of this sequence so we use a symbol language of around 100000 tokens that we then use to represent all of the data sequence in an optimized manner.\n\n- Interesting _bit_ around how binary isn't a good language for tokenization since we want to optimzie on the length of the sequnce and binary gives us very long sequences since it uses **8 bits** to represent a character/letter. So to represent 5000 chars using binary would create a sequnce that is 40000 chars long. If we were to use bytes for each would that would again be 5000 tokens and if we were to use GPT tokens it shrinks the sequnce to 1300 tokens.\n\n## Step 3 - Neural Network Training\n\n- Here we grab an arbitrary length window of tokens and feed that as input to the neural network, this untrained neural network then outputs the probability of all the 100000 tokens being the next word in the sequence of that window. Since we know what the next word is ourselves, we can then use that to label/tune the neural network using mathematics.\n\n- Obviously this with not just one window but with all the tokens. We grab different windows and parallel process them in batches and then tune/label them and train them again till the output probabilities of the models align with the statisitical patters in our training set.\n\n- Also in theory, we could have an infinite window length but in practive we cap it out at around 8000.\n\n### Neural Network Internals\n\n- The neural network has billions of paramters, but for the sake of our conversation, let's say that there are only 6 params that have random values like [0.5, 1, -1, 3, 0.8, 0.2] assigned to them initally. And if we run this neural network with these random parameter values; it makes random predictions. But it is through the process of iteratively updating the network or training the network that the setting of the parameters get tuned to the \"correct\" values and it's probability outputs get more in tune with the statistical patterns in our dataset.\n\n### Inference\n\n- Once the neural network has been trained and it's parameters have been tuned, the next _step_ is *Inference*. Inference involves predicting the next token using the provided input token. The prediction is based on the probabilities / output the network generates using the trained parameters. It basically samples a token from the distribution based on the probablities and outputs that token as the next one in the sequence and then re-feeds that into the input as the next sequnce.","n":0.04}}},{"i":55,"$":{"0":{"v":"3Blue1Brown","n":1},"1":{"v":"\n\n[[ML.AK.Deep dive into Chat GPT]]","n":0.447}}}]}
