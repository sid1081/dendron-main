
# Deep dive into LLMS like Chat GPT

## Step 1 - Pretraining: download and pre-process the internet

- This is the part where we scrape the internet for colossla amounts of quality text. This is something that **every** LLM company must have done. **Fineweb** is a dataset that is equivalent to the text dataset these companies assimmilate.

- In order to collect all of this data, **Hugging Face**(the company that created Fineweb) started with using data from **Common Crawl** which has been crawling the internet since 2007. This data then goes through a _pipeline_ where we filter through various different items - like filter through websites on the blocklist, get rid of all of the HTML markup, get rid of text from items in the navigation panel on a website, so on and so forth.

- At the end of this pipeline, you will have a clean high quality text data set.

## Step 2 - Tokenization

- Here we convert the sequence of text that was generated by concatenating the the text dataset from Step 1, into a sequence of _tokens_ or _symbols_. The reason we need to do this is because the neural network takes as input a one dimensional sequence of tokens or symbols and we want to optimize on the length of this sequence so we use a symbol language of around 100000 tokens that we then use to represent all of the data sequence in an optimized manner.

- Interesting _bit_ around how binary isn't a good language for tokenization since we want to optimzie on the length of the sequnce and binary gives us very long sequences since it uses **8 bits** to represent a character/letter. So to represent 5000 chars using binary would create a sequnce that is 40000 chars long. If we were to use bytes for each would that would again be 5000 tokens and if we were to use GPT tokens it shrinks the sequnce to 1300 tokens.

## Step 3 - Neural Network Training

- Here we grab an arbitrary length window of tokens and feed that as input to the neural network, this untrained neural network then outputs the probability of all the 100000 tokens being the next word in the sequence of that window. Since we know what the next word is ourselves, we can then use that to label/tune the neural network using mathematics.

- Obviously this with not just one window but with all the tokens. We grab different windows and parallel process them in batches and then tune/label them and train them again till the output probabilities of the models align with the statisitical patters in our training set.

- Also in theory, we could have an infinite window length but in practive we cap it out at around 8000.

### Neural Network Internals

- The neural network has billions of paramters, but for the sake of our conversation, let's say that there are only 6 params that have random values like [0.5, 1, -1, 3, 0.8, 0.2] assigned to them initally. And if we run this neural network with these random parameter values; it makes random predictions. But it is through the process of iteratively updating the network or training the network that the setting of the parameters get tuned to the "correct" values and it's probability outputs get more in tune with the statistical patterns in our dataset.

### Inference

- Once the neural network has been trained and it's parameters have been tuned, the next _step_ is *Inference*. Inference involves predicting the next token using the provided input token. The prediction is based on the probabilities / output the network generates using the trained parameters. It basically samples a token from the distribution based on the probablities and outputs that token as the next one in the sequence and then re-feeds that into the input as the next sequnce.