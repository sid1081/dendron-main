<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-main/favicon.ico"/><title>Deep dive into Chat GPT</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Knowledge Space"/><meta property="og:title" content="Deep dive into Chat GPT"/><meta property="og:description" content="Personal Knowledge Space"/><meta property="og:url" content="https://sid1081.github.io/dendron-main/notes/qufzkdfqezix7zzqwzdxja2/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2/19/2025"/><meta property="article:modified_time" content="2/28/2025"/><link rel="canonical" href="https://sid1081.github.io/dendron-main/notes/qufzkdfqezix7zzqwzdxja2/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-main/_next/static/css/c3b5c8984096a078.css" as="style"/><link rel="stylesheet" href="/dendron-main/_next/static/css/c3b5c8984096a078.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-main/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-main/_next/static/chunks/webpack-0caed3b86e25c9d3.js" defer=""></script><script src="/dendron-main/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-main/_next/static/chunks/main-af01d34aaf1cbbc0.js" defer=""></script><script src="/dendron-main/_next/static/chunks/pages/_app-f3ffa24acc1f5021.js" defer=""></script><script src="/dendron-main/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-main/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-main/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-main/_next/static/j1tAvk9N20cOpobDNsQ-j/_buildManifest.js" defer=""></script><script src="/dendron-main/_next/static/j1tAvk9N20cOpobDNsQ-j/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="deep-dive-into-chat-gpt"><a aria-hidden="true" class="anchor-heading icon-link" href="#deep-dive-into-chat-gpt"></a>Deep dive into Chat GPT</h1>
<h1 id="deep-dive-into-llms-like-chat-gpt"><a aria-hidden="true" class="anchor-heading icon-link" href="#deep-dive-into-llms-like-chat-gpt"></a>Deep dive into LLMS like Chat GPT</h1>
<h2 id="step-1---pretraining-download-and-pre-process-the-internet"><a aria-hidden="true" class="anchor-heading icon-link" href="#step-1---pretraining-download-and-pre-process-the-internet"></a>Step 1 - Pretraining: download and pre-process the internet</h2>
<ul>
<li>
<p>This is the part where we scrape the internet for colossla amounts of quality text. This is something that <strong>every</strong> LLM company must have done. <strong>Fineweb</strong> is a dataset that is equivalent to the text dataset these companies assimmilate.</p>
</li>
<li>
<p>In order to collect all of this data, <strong>Hugging Face</strong>(the company that created Fineweb) started with using data from <strong>Common Crawl</strong> which has been crawling the internet since 2007. This data then goes through a <em>pipeline</em> where we filter through various different items - like filter through websites on the blocklist, get rid of all of the HTML markup, get rid of text from items in the navigation panel on a website, so on and so forth.</p>
</li>
<li>
<p>At the end of this pipeline, you will have a clean high quality text data set.</p>
</li>
</ul>
<h2 id="step-2---tokenization"><a aria-hidden="true" class="anchor-heading icon-link" href="#step-2---tokenization"></a>Step 2 - Tokenization</h2>
<ul>
<li>
<p>Here we convert the sequence of text that was generated by concatenating the the text dataset from Step 1, into a sequence of <em>tokens</em> or <em>symbols</em>. The reason we need to do this is because the neural network takes as input a one dimensional sequence of tokens or symbols and we want to optimize on the length of this sequence so we use a symbol language of around 100000 tokens that we then use to represent all of the data sequence in an optimized manner.</p>
</li>
<li>
<p>Interesting <em>bit</em> around how binary isn't a good language for tokenization since we want to optimzie on the length of the sequnce and binary gives us very long sequences since it uses <strong>8 bits</strong> to represent a character/letter. So to represent 5000 chars using binary would create a sequnce that is 40000 chars long. If we were to use bytes for each would that would again be 5000 tokens and if we were to use GPT tokens it shrinks the sequnce to 1300 tokens.</p>
</li>
</ul>
<h2 id="step-3---neural-network-training"><a aria-hidden="true" class="anchor-heading icon-link" href="#step-3---neural-network-training"></a>Step 3 - Neural Network Training</h2>
<ul>
<li>
<p>Here we grab an arbitrary length window of tokens and feed that as input to the neural network, this untrained neural network then outputs the probability of all the 100000 tokens being the next word in the sequence of that window. Since we know what the next word is ourselves, we can then use that to label/tune the neural network using mathematics.</p>
</li>
<li>
<p>Obviously this with not just one window but with all the tokens. We grab different windows and parallel process them in batches and then tune/label them and train them again till the output probabilities of the models align with the statisitical patters in our training set.</p>
</li>
<li>
<p>Also in theory, we could have an infinite window length but in practive we cap it out at around 8000.</p>
</li>
</ul>
<h3 id="neural-network-internals"><a aria-hidden="true" class="anchor-heading icon-link" href="#neural-network-internals"></a>Neural Network Internals</h3>
<ul>
<li>The neural network has billions of paramters, but for the sake of our conversation, let's say that there are only 6 params that have random values like [0.5, 1, -1, 3, 0.8, 0.2] assigned to them initally. And if we run this neural network with these random parameter values; it makes random predictions. But it is through the process of iteratively updating the network or training the network that the setting of the parameters get tuned to the "correct" values and it's probability outputs get more in tune with the statistical patterns in our dataset.</li>
</ul>
<h3 id="inference"><a aria-hidden="true" class="anchor-heading icon-link" href="#inference"></a>Inference</h3>
<ul>
<li>Once the neural network has been trained and it's parameters have been tuned, the next <em>step</em> is <em>Inference</em>. Inference involves predicting the next token using the provided input token. The prediction is based on the probabilities / output the network generates using the trained parameters. It basically samples a token from the distribution based on the probablities and outputs that token as the next one in the sequence and then re-feeds that into the input as the next sequnce.</li>
</ul>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/dendron-main/notes/dsryuppzxk9dtugblsu7a23">3Blue1Brown</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#deep-dive-into-llms-like-chat-gpt" title="Deep dive into LLMS like Chat GPT">Deep dive into LLMS like Chat GPT</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#step-1---pretraining-download-and-pre-process-the-internet" title="Step 1 - Pretraining: download and pre-process the internet">Step 1 - Pretraining: download and pre-process the internet</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#step-2---tokenization" title="Step 2 - Tokenization">Step 2 - Tokenization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#step-3---neural-network-training" title="Step 3 - Neural Network Training">Step 3 - Neural Network Training</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#neural-network-internals" title="Neural Network Internals">Neural Network Internals</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#inference" title="Inference">Inference</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"qufzkdfqezix7zzqwzdxja2","title":"Deep dive into Chat GPT","desc":"","updated":1740783689199,"created":1740002114964,"url":"https://www.youtube.com/watch?v=7xTGNNLPyMI\u0026t=1s\u0026ab_channel=AndrejKarpathy","custom":{"url":"https://www.youtube.com/watch?v=7xTGNNLPyMI\u0026t=1s\u0026ab_channel=AndrejKarpathy"},"fname":"ML.AK.Deep dive into Chat GPT","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"0842f0671b6561f150f4676eb78888d1","links":[{"from":{"fname":"ML.3Blue1Brown","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":3,"column":1,"offset":2},"end":{"line":3,"column":34,"offset":35},"indent":[]},"value":"ML.AK.Deep dive into Chat GPT","alias":"ML.AK.Deep dive into Chat GPT"}],"anchors":{"deep-dive-into-llms-like-chat-gpt":{"type":"header","text":"Deep dive into LLMS like Chat GPT","value":"deep-dive-into-llms-like-chat-gpt","line":9,"column":0,"depth":1},"step-1---pretraining-download-and-pre-process-the-internet":{"type":"header","text":"Step 1 - Pretraining: download and pre-process the internet","value":"step-1---pretraining-download-and-pre-process-the-internet","line":11,"column":0,"depth":2},"step-2---tokenization":{"type":"header","text":"Step 2 - Tokenization","value":"step-2---tokenization","line":19,"column":0,"depth":2},"step-3---neural-network-training":{"type":"header","text":"Step 3 - Neural Network Training","value":"step-3---neural-network-training","line":25,"column":0,"depth":2},"neural-network-internals":{"type":"header","text":"Neural Network Internals","value":"neural-network-internals","line":33,"column":0,"depth":3},"inference":{"type":"header","text":"Inference","value":"inference","line":37,"column":0,"depth":3}},"children":[],"parent":"kndcs9cayrizdzefp1veykh","data":{}},"body":"\u003ch1 id=\"deep-dive-into-chat-gpt\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#deep-dive-into-chat-gpt\"\u003e\u003c/a\u003eDeep dive into Chat GPT\u003c/h1\u003e\n\u003ch1 id=\"deep-dive-into-llms-like-chat-gpt\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#deep-dive-into-llms-like-chat-gpt\"\u003e\u003c/a\u003eDeep dive into LLMS like Chat GPT\u003c/h1\u003e\n\u003ch2 id=\"step-1---pretraining-download-and-pre-process-the-internet\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#step-1---pretraining-download-and-pre-process-the-internet\"\u003e\u003c/a\u003eStep 1 - Pretraining: download and pre-process the internet\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThis is the part where we scrape the internet for colossla amounts of quality text. This is something that \u003cstrong\u003eevery\u003c/strong\u003e LLM company must have done. \u003cstrong\u003eFineweb\u003c/strong\u003e is a dataset that is equivalent to the text dataset these companies assimmilate.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn order to collect all of this data, \u003cstrong\u003eHugging Face\u003c/strong\u003e(the company that created Fineweb) started with using data from \u003cstrong\u003eCommon Crawl\u003c/strong\u003e which has been crawling the internet since 2007. This data then goes through a \u003cem\u003epipeline\u003c/em\u003e where we filter through various different items - like filter through websites on the blocklist, get rid of all of the HTML markup, get rid of text from items in the navigation panel on a website, so on and so forth.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt the end of this pipeline, you will have a clean high quality text data set.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"step-2---tokenization\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#step-2---tokenization\"\u003e\u003c/a\u003eStep 2 - Tokenization\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHere we convert the sequence of text that was generated by concatenating the the text dataset from Step 1, into a sequence of \u003cem\u003etokens\u003c/em\u003e or \u003cem\u003esymbols\u003c/em\u003e. The reason we need to do this is because the neural network takes as input a one dimensional sequence of tokens or symbols and we want to optimize on the length of this sequence so we use a symbol language of around 100000 tokens that we then use to represent all of the data sequence in an optimized manner.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInteresting \u003cem\u003ebit\u003c/em\u003e around how binary isn't a good language for tokenization since we want to optimzie on the length of the sequnce and binary gives us very long sequences since it uses \u003cstrong\u003e8 bits\u003c/strong\u003e to represent a character/letter. So to represent 5000 chars using binary would create a sequnce that is 40000 chars long. If we were to use bytes for each would that would again be 5000 tokens and if we were to use GPT tokens it shrinks the sequnce to 1300 tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"step-3---neural-network-training\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#step-3---neural-network-training\"\u003e\u003c/a\u003eStep 3 - Neural Network Training\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHere we grab an arbitrary length window of tokens and feed that as input to the neural network, this untrained neural network then outputs the probability of all the 100000 tokens being the next word in the sequence of that window. Since we know what the next word is ourselves, we can then use that to label/tune the neural network using mathematics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eObviously this with not just one window but with all the tokens. We grab different windows and parallel process them in batches and then tune/label them and train them again till the output probabilities of the models align with the statisitical patters in our training set.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAlso in theory, we could have an infinite window length but in practive we cap it out at around 8000.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"neural-network-internals\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#neural-network-internals\"\u003e\u003c/a\u003eNeural Network Internals\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe neural network has billions of paramters, but for the sake of our conversation, let's say that there are only 6 params that have random values like [0.5, 1, -1, 3, 0.8, 0.2] assigned to them initally. And if we run this neural network with these random parameter values; it makes random predictions. But it is through the process of iteratively updating the network or training the network that the setting of the parameters get tuned to the \"correct\" values and it's probability outputs get more in tune with the statistical patterns in our dataset.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"inference\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#inference\"\u003e\u003c/a\u003eInference\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOnce the neural network has been trained and it's parameters have been tuned, the next \u003cem\u003estep\u003c/em\u003e is \u003cem\u003eInference\u003c/em\u003e. Inference involves predicting the next token using the provided input token. The prediction is based on the probabilities / output the network generates using the trained parameters. It basically samples a token from the distribution based on the probablities and outputs that token as the next one in the sequence and then re-feeds that into the input as the next sequnce.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/dendron-main/notes/dsryuppzxk9dtugblsu7a23\"\u003e3Blue1Brown\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"root","title":"root","desc":"","updated":1746040151076,"created":1595961348801,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"3338f3f44ea53fff248fd70b6ad85170","links":[],"anchors":{},"children":["oz8e6a05bjgd3cbtru3hqe2","c1bs7wsjfbhb0zipaywqv1","3x4fmd16chquma6kovvfatj","gdjyzsq56qgyj5t5uktrsgu","69h59jt3vyfdg55g7gqc95d","wz9z8az9m9aq5132p7d10co","l1lcmphk4ubvu1aiqhcs7ib","bd3mupfj68nrgnjejss1qqh","1tlnuf6o2f3qawzg799ilzk","z5mfdumpr0cv3ceh60flq8n","filwymswyiwdojrwinvy5xd"],"parent":null,"data":{},"body":"\nSiddharth's root\n\nThis vault contains all the verticals to climb."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false,"enableHandlebarTemplates":true,"enableSmartRefs":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableMermaid":true},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal Knowledge Space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"search","siteUrl":"https://sid1081.github.io","assetsPrefix":"/dendron-main","enableMermaid":true,"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"qufzkdfqezix7zzqwzdxja2"},"buildId":"j1tAvk9N20cOpobDNsQ-j","assetPrefix":"/dendron-main","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>